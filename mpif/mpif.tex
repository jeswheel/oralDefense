\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usetheme{gotham}

	\usepackage{standalone}
	\usepackage{tikz}
	\usepackage{pgfplots}
	\usepackage{tabularray} % Typeset tabulars and arrays (contains equivalent of longtable, booktabs and dcolumn at least)
		\UseTblrLibrary{booktabs} % to load extra commands from booktabs
	\usepackage{changepage}
	\usepackage{minted}
		\definecolor{codeback}{rgb}{0.90,0.91,0.92}
		\definecolor{codebackdark}{rgb}{0.10,0.11,0.12}

	\newcommand{\famName}[1]{\textsc{#1}}
	\newcommand{\themename}{\textbf{\textsc{Gotham}}}



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\section{The Marginalized Panel Iterated Filter (MPIF)}

\begin{frame}{Motivation}
Often we have a collection of related time series called, called \emph{panel data}.
We want to make inference using the entire collection, not just on each time series.

Examples
  \begin{itemize}
    \item Model for disease outbreaks of the same disease, different locations (hospitals / cities) \citep{lee20}.
    \item Experiments / observational studies on ecological populations \citep{searle16}.
    \item Longitudinal studies using within-host dynamic models \citep{ranjeva17}.
  \end{itemize}
  
  Mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability.
  
  % This suggests that the practical difficulties for existing procedures. 
\end{frame}

\begin{frame}{Panel models}
  Measurements for unit $u$ taken at times $t_{u, 1:N_u}$. Observed and latent process at these times denoted $Y_{u, n}$ and $X_{u, n}$, respectively.
  
  Each unit $u$ defines an independent POMP model, the entire collection of models is a PanelPOMP.
  
  $$
  \mathcal{L}(\paramVec; \bm{y}^*) = \int \prod_{u = 1}^U f_{X_{u, 0}}\big(x_{u, 0};\, \paramVec\big)\prod_{n = 1}^{N_u} f_{X_{u, n}|X_{u, n-1}}\big(x_{u, n}|x_{u, n-1}; \, \paramVec\big)f_{Y_{u, n}|X_{u, n}}\big(y_{u, n}|x_{u, n}; \, \paramVec\big) dx_{1:U,0:N_u}.
  $$
  
  The parameter vector $\theta$ has shared components $\phi$, and unit specific components $\psi_{1:U}$.
  $$\theta = (\phi, \psi_{1:U})$$.
  
\end{frame}

\begin{frame}{The problem}

  \begin{itemize}
    \item Particle filters work in low-dimensions, can be applied independently to units.
    \item Iterated filtering (IF) is an extension used to perform maximum likelihood estimation \citep{ionides15}.
    \item IF introduces dependence because of shared $\theta$, making it a high-dimensional problem. 
  \end{itemize}

\end{frame}

\begin{frame}{Background: Data cloning and iterated filtering}
  
  IF is a special type of Data cloning \citep{lele07}, which is repeated applications of Bayes rule. 
  
  Denote $\pi_i(\theta)$ as the posterior distribution of the parameter vector $\theta$ after the $i$th Bayesian update, and $\mathcal{L}(\paramVec; \bm{y}^*)$ as the likelihood
\begin{align*}
\pi_1(\theta) &\propto \mathcal{L}(\paramVec; \bm{y}^*)\pi_0(\theta), \\
\pi_2(\theta) &\propto \mathcal{L}(\paramVec; \bm{y}^*)\pi_1(\theta) \propto \mathcal{L}(\paramVec; \bm{y}^*)^2\pi_0(\theta),\\
&\vdots\\
\pi_m(\theta) &\propto \mathcal{L}(\paramVec; \bm{y}^*)^m\pi_0(\theta).
\end{align*}
If we let $m\rightarrow \infty$, the effect of the initial prior distribution diminishes, and the $m$th posterior has all of its mass centered at the MLE.

\end{frame}

\begin{frame}{Iterated filtering}
  Loosely speaking, iterated filtering is just data cloning with the additional pieces: 
  \begin{enumerate}
    \item Likelihood cannot be evaluated exactly, it's approximated using particle filters. 
    \item At each time-step, the parameter particles are perturbed.
    \item Parameter particles reweighted using conditional log-likelihoods.
  \end{enumerate}
  The perturbation of parameters is necessary to avoid particle depletion, a known problem with particle filters + Bayesian inference \citep{chen24}.
  
  The perturbations introduce a loss of information \citep{liu01}, so are decreased over the cloning iteration.
\end{frame}

\begin{frame}{PIF Theory}
  The panel iterated filter (PIF) is a type of IF, mitigating information loss \citep{breto20}. 
  It has been successfully used to estimate the MLE previously \citep[e.g.,][]{domeyer22}.
  
  \begin{theorem}[Chapter 4, Theorem 1]
    Extends theory of \citet{chen24} to panel models. Denote the output of the PIF algorithm as $\Theta_{1:J}^{(M)}$.
  Then there exists some positive sequences $\{C_M\}_{M \geq 1}$ and $\{\epsilon_M\}_{M \geq 1}$ where $\lim_{M \rightarrow \infty}\epsilon_M = 0$ such that for all $(J, M) \in \mathbb{N}^2$,
  $$
  E\bigg[\Big|\frac{1}{J}\sum_{i=1}^J \Theta_j^{(M)} - \hat{\theta}\Big|_2\bigg] \leq \frac{C_M}{\sqrt{J}} + \epsilon_M
  $$
  \end{theorem}
  
\end{frame}

\begin{frame}{Proof sketch and discussion}
  Conditions:
  \begin{itemize}
    \item On parameter space $\Theta$: Compact, corners not too "sharp" \citep[regular compact set, Def~1 of][]{chen24}.
    \item Regularity conditions on the model (positive and finite likelihood, densities are smooth functions of $\theta$). 
    \item Conditions on the parameter perturbations (type of perturbations, and cooling schedule). 
  \end{itemize}
  Proof Sketch: 
  \begin{itemize}
    \item \citet{chen24} provides framework for convergence of IF of POMP models; we write a general PanelPOMP as a POMP model, and PIF is a special case of IF for these models. 
  \end{itemize}
\end{frame}

\begin{frame}{PIF: a tradeoff}
  Iterated filtering takes data one observation at a time. 
  In the panel setting, this means we process by unit $u$. 
  Ignoring perturbations, the algorithm iterates over $(m, u)$: 
  \begin{align}
    \pi_{m, u}(\theta) &\propto \mathcal{L}_u(\paramVec; y_u^*)\, \pi_{m, u-1}(\theta) = \mathcal{L}_u(\phi, \psi_u; y_u^*)\, \pi_{m, u-1}(\theta), \label{eq:PIFupdate}
  \end{align}
    Using $\pi_{0, 0}(\theta)$ as the initial prior distribution. Even if prior is independent, we get dependence by iterating Eq.~\ref{eq:PIFupdate} over $u$. Two options of iterated filtering: 
    \begin{itemize}
      \item Perturb all parameters at each time step (high loss of information). 
      \item When using data from unit $u$, only perturb $\phi$ and $\psi_u$ (the PIF algorithm, high particle depletion). 
    \end{itemize}
\end{frame}

\begin{frame}{MPIF motivation}
  If the last prior $\pi_{m, u-1}(\theta)$ has independence: $\pi_{m, u-1}(\theta) = f(\psi_{-u})g(\phi, \psi_{u})$, then there is no need to resample particles $\psi_{-u}$. This would avoid the particle depletion \emph{and} loss-of-information.
  
  This leads to marginalized data cloning (repeating Eqs.~\ref{eq:margBayes}--\ref{eq:MPIFupdate}). 
  
  \begin{align}
\tilde{\pi}_{m, u}(\theta) &\propto \mathcal{L}_{u}(y^*_u;\, \phi, \psi_u)\, \pi_{m, u-1}(\theta) \label{eq:margBayes}\\
\pi_{m, u}(\theta) &\propto \int \! \tilde{\pi}_{m, u}(\theta) \, d\phi \, d\psi_u \, \times \int \! \tilde{\pi}_{m, u}(\theta) \, d\psi_{-u} \label{eq:MPIFupdate}.
\end{align}
\end{frame}

\begin{frame}{Marginalized Bayes: Guassian figure}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/figDC-1} 

}


\end{knitrout}

\end{frame}

\begin{frame}{MPIF theory}

  \begin{itemize}
    \item Like IF extends data cloning, the MPIF algorithm extends this marginalized data cloning.
    \item Existing theory for IF algorithms cannot readily be extended to MPIF, because of the non-linearity introduced by the marginalization step. 
    \item A natural first question is whether or not marginalized data cloning converges.
    \begin{itemize}
      \item Unfortunately, a few toy examples suggests not always.
    \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}{Marginalized data cloning: Gaussian likelihoods}
  Convergence is explored via Gaussian likelihoods.
  The properties of this special case is relevant to the broader class of models that is well approximated by Gaussian models, (e.g., local asymptotic normality \citep{lecam00}).
  
  \begin{theorem}[Chapter 4, Theorem 2]
    Let $\mathcal{L}_u(y_u^*; \, \phi, \psi_u)$ be the likelihood that corresponds to a Gaussian distribution with mean $(\phi^*, \psi_u^*)$ and precision $\Lambda_u^* \in \R^{2\times2}$. Under suitable conditions on the matrices $\Lambda^*_u$, then if the initial prior density is Gaussian, then the density of the $m$th iteration of Eq.~\ref{eq:MPIFupdate} converges to a point mass at the MLE $(\phi^*, \psi_1^*, \ldots, \psi^*_U)$ as $m\rightarrow \infty$.
  \end{theorem}
\end{frame}

\begin{frame}{Gaussian Theory: Proof sketch}

\begin{itemize}
  \item Gaussian priors + Gaussian likelihoods imply Gaussian posteriors.
  \item Transform data so likelihood is centered at zero.
  \item The marginalization step only modifies the covariance, setting off-diagonal terms to zero. Conditions ensure this loss of information is not too large.
  \item Diagonals of covariance shrink to zero asymptotically at rate $1/m$.
  \item Each unit-iteration updates the $\phi$ and $\psi_u$ components of $\mu_m$.
  \item $\mu_m = \big(\prod_{i = 1}^m\prod_{u = 1}^U A_{u, i}\big) \mu_0 = \big(\prod_{i = 1}^m P_m\big) \mu_0$, with $\|P_m\|_2 = 1 - \epsilon_m/m + o(1/m)$, with $\epsilon_m$ positive, bounded.
\end{itemize}

\end{frame}

\begin{frame}{Convergence with perturbations}

    \begin{theorem}[Chapter 4, Corollary 1]
    Considering the same setup as Theorem~2 (Chapter~4), if parameters are perturbed prior performing the Bayes update at each step using Gaussian additive noise with mean $0$ and covariance $\sigma^2_m\Sigma_0$, then if $\sigma_m^2 = o(1/m)$, then the algorithm still converges to the MLE as $m\rightarrow \infty$. 
  \end{theorem}
  
  \begin{itemize}
    \item This corollary is motivated by heuristic: a more dispersed prior typically results in a posterior distribution that more closely resembles the likelihood function.
    \item Adding Gaussian noise at each step results in larger updates towards to MLE at each step.
    \item Heuristically, convergence of unperturbed case implies convergence of perturbed case, if perturbation variance decreases to zero.
  \end{itemize}

\end{frame}

\begin{frame}{The MPIF advantage: improved particle representation}

  One perspective is that the marginalization adds a small amount of bias in each intermediate posterior distribution.
  
  \begin{itemize}
    \item Theorem~2 (Chapter 4) gives formal conditions where the bias at each step is small enough that the combined iterations still converge to the MLE (no bias).
    \item The advantage of MPIF, however, is improved particle representations. In this case, MPIF may still be preferable even if there is bias in final estimate.
  \end{itemize}
  
\end{frame}

\begin{frame}{Bias-Variance tradeoff}
  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.65\maxwidth]{figure/unnamed-chunk-1-1} 

}


\end{knitrout}
  
\end{frame}

\begin{frame}{Gompertz population model: high-dimensional space}

The MPIF algorithm is demonstrated on data simulated from a stochastic population model. We have $U$ ranging from $5--2500$, and $N_u = N \in \{20, 50, 100\}$.
\begin{itemize}
  \item $X_{u, n+1} = K^{1- \exp{r_u}}_u \epsilon_{u, n}$, $\epsilon_{u, n} \sim N(0, \sigma^2_u)$. 
  \item $Y_{u, n} | X_{u, n} \sim N\big(\log X_{u, n}, \tau^2_u\big)$
  \item Fix $K_u = 1$, $X_{u, 0} = 1$. Estimate $\sigma_u^2 = \sigma^2 = 0.01$, $r_u = r = 0.1$, and $\tau_u^2 = 0.01$ for all $u$.  
\end{itemize}

This is log-linear Gaussian, so exact likelihood can be computed \citep{kalman60}. Parameter choices match \citet{breto20}. 

\end{frame}

\begin{frame}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/gompFig1-1} 

}


\end{knitrout}
\end{frame}

\begin{frame}{Discussion}
  \begin{itemize}
    \item In all tested models, the MPIF algorithm outperforms PIF, especially as the number of units $U$ is large, and number of unit-specific parameters is large.
    \item Even poor performing replicates of MPIF often outperform PIF, despite having weaker theoretical guarantees.
    \item Improved performance is a result of reduced particle depletion.
    \item Stronger theory for MPIF is available in some special cases, each covered by Theorem~1 of Chapter~4:
    \begin{itemize}
      \item No shared parameters: there is no parameter dependence, and the algorithm is equivalent to performing IF2 independently to each model. 
      \item No unit-specific parameters: the algorithm is the same as PIF, as resampling unit-specific parameters is not needed.
    \end{itemize}
  \end{itemize}
\end{frame}

\end{document}
%EoF
