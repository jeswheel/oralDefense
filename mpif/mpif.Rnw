\documentclass[aspectratio=169]{beamer}
\usetheme{gotham}

	\usepackage{standalone}
	\usepackage{tikz}
	\usepackage{pgfplots}
	\usepackage{tabularray} % Typeset tabulars and arrays (contains equivalent of longtable, booktabs and dcolumn at least)
		\UseTblrLibrary{booktabs} % to load extra commands from booktabs
	\usepackage{changepage}
	\usepackage{minted}
	\usepackage[ruled,noline,linesnumbered]{algorithm2e}
		\definecolor{codeback}{rgb}{0.90,0.91,0.92}
		\definecolor{codebackdark}{rgb}{0.10,0.11,0.12}

	\newcommand{\famName}[1]{\textsc{#1}}
	\newcommand{\themename}{\textbf{\textsc{Gotham}}}

<<MPIFSetup, include=FALSE,echo=FALSE,results='hide'>>=

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = 'center',
  dev = 'cairo_pdf'
)

library(knitr)       # Necessary for Rnw file
library(tidyverse)
library(panelPomp)        # Used for the bake function
library(latex2exp)
# source("mpif_example/functions.R")

# Setting black and white ggplot2 theme for entire document.
theme_set(
  theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5)
    )
)

calculate_ellipse <- function(mu, sigma) {
  radius <- sqrt(2 * stats::qf(0.95, 2, Inf))
  inner <- function(i) {
    chol_decomp <- chol(sigma[[i]])
    angles <- (0:100) * 2 * pi / 100
    unit.circle <- cbind(cos(angles), sin(angles))
    ellipse <- t(mu[i, ] + radius * t(unit.circle %*% chol_decomp))
    ellipse <- as.data.frame(ellipse)
    colnames(ellipse) <- c("X1","X2")
    ellipse$group <- i
    ellipse
  }
  do.call(rbind, purrr::map(1:length(sigma), inner))
}

MARG_COLS <- c("#c05502", "#178a68")

#' Gaussian-Gaussian, single unit, data cloning algorithm.
#'
#' This function performs data-cloning on the special case with Gaussian
#' likelihood and prior, with the MLE centered at zero.
#'
#' @param Lambda Covariance matrix of likelihood.
#' @param Prior Prior distribution for parameters.
#' @param M Number of iterations.
#'
#' @return
GG_DC <- function(Lambda, Prior, M) {

  is_pd <- matrixcalc::is.positive.definite(
    Lambda
  )

  if (!is_pd) stop("Lambda must be a postive definite matrix")

  # A single step of the data cloning algorithm
  DC_update <- function(prior) {
    new_precision <- prior$precision + Lambda
    new_mean <- solve(new_precision) %*% (prior$precision %*% prior$mean)

    new_prior <- list(
      'mean' = new_mean,
      'precision' = new_precision
    )
    new_prior
  }

  all_DC_dists <- list()
  all_DC_dists[[1]] <- Prior

  priorDC <- Prior

  for (i in 1:M) {
    priorDC <- DC_update(priorDC)
    all_DC_dists[[i + 1]] <- priorDC
  }

  all_DC_dists
}

#' Gaussian-Gaussian, single unit, perturbed data cloning algorithm.
#'
#' This function performs perturbed data-cloning on the special case with Gaussian
#' likelihood and prior, with the MLE centered at zero.
#'
#' @param Lambda Covariance matrix of likelihood.
#' @param Prior Prior distribution for parameters.
#' @param M Number of iterations.
#'
#' @return
GG_PDC <- function(Lambda, Prior, M, init_noise = 1/4) {

  is_pd <- matrixcalc::is.positive.definite(
    Lambda
  )

  if (!is_pd) stop("Lambda must be a postive definite matrix")

  # A single step of the data cloning algorithm
  PDC_update <- function(prior, m, tau0 = 1/init_noise) {

    # Goes to infinity quite fast (i.e., the variance goes to zero fast)
    var_eps <- diag(1 / (tau0 * m), nrow = 2, ncol = 2)
    tmp_prior <- prior

    # Add random noise to prior, with variance 1 / (tau0 * m^2) -> 0
    tmp_prior$precision <- solve(solve(prior$precision) + var_eps)

    # Now do DC.
    new_precision <- tmp_prior$precision + Lambda
    new_mean <- solve(new_precision) %*% (tmp_prior$precision %*% tmp_prior$mean)

    new_prior <- list(
      'mean' = new_mean,
      'precision' = new_precision
    )
    new_prior
  }

  all_DC_dists <- list()
  all_DC_dists[[1]] <- Prior

  priorDC <- Prior

  for (i in 1:M) {
    priorDC <- PDC_update(priorDC, m = i, tau0 = 1/init_noise)
    all_DC_dists[[i + 1]] <- priorDC
  }

  all_DC_dists
}

#' Gaussian-Gaussian, single unit, marginalized data cloning algorithm.
#'
#' This function performs marginalized data-cloning on the special case with
#' Gaussian likelihood and prior, with the MLE centered at zero.
#'
#' @param Lambda Covariance matrix of likelihood.
#' @param Prior Prior distribution for parameters.
#' @param M Number of iterations.
#'
#' @return
GG_MDC <- function(Lambda, Prior, M) {

  prior <- Prior

  is_pd <- matrixcalc::is.positive.definite(
    Lambda
  )

  if (!is_pd) stop("Lambda must be a postive definite matrix")

  all_MDC_dists <- list()
  all_MDC_dists[[1]] <- prior

  for (i in 1:M) {
    prior <- all_MDC_dists[[i]]
    # prior$precision <- diag(diag(prior$precision))
    new_prior <- GG_DC(Lambda, prior, 1)[[2]]
    new_prior$precision <- diag(diag(new_prior$precision))
    all_MDC_dists[[i+1]] <- new_prior
    # prior <- all_MDC_dists[[i+1]]
  }
  all_MDC_dists
}

GG_PMDC <- function(Lambda, Prior, M, init_noise = 1/4) {

  prior <- Prior

  is_pd <- matrixcalc::is.positive.definite(
    Lambda
  )

  if (!is_pd) stop("Lambda must be a postive definite matrix")

  all_MDC_dists <- list()
  all_MDC_dists[[1]] <- prior

  for (i in 1:M) {
    prior <- all_MDC_dists[[i]]

    # Goes to infinity quite fast (i.e., the variance goes to zero fast)
    var_eps <- diag(init_noise / i, nrow = 2, ncol = 2)
    tmp_prior <- prior

    # Add random noise to prior, with variance 1 / (tau0 * m^2) -> 0
    tmp_prior$precision <- solve(solve(prior$precision) + var_eps)

    # prior$precision <- diag(diag(prior$precision))
    new_prior <- GG_DC(Lambda, tmp_prior, 1)[[2]]
    new_prior$precision <- diag(diag(new_prior$precision))
    all_MDC_dists[[i+1]] <- new_prior
    # prior <- all_MDC_dists[[i+1]]
  }
  all_MDC_dists
}
@

\begin{document}

\section{The Marginalized Panel Iterated Filter (MPIF)}

\begin{frame}{Motivation}
Collection of related time series are called \emph{panel data}.
We want to make inference using the entire collection, not just on each time series.

Examples:
  \begin{itemize}
    \item Model for disease outbreaks of the same disease, different locations (hospitals / cities) \citep{lee20}.
    \item Experiments / observational studies on ecological populations \citep{searle16}.
    \item Longitudinal studies using within-host dynamic models \citep{ranjeva17}.
  \end{itemize}
  
  Mechanistic models seldom used for panel data, despite widespread availability.
  
  % This suggests that the practical difficulties for existing procedures. 
\end{frame}

\begin{frame}{Panel models}
  Measurements taken at $u \in 1:U$ units. Times are $t_{u, 1:N_u}$. Observed and latent process at these times denoted $Y_{u, n}$ and $X_{u, n}$, respectively.
  
  Each unit $u$ defines an independent POMP model, the entire collection of models is a PanelPOMP.
  
  $$
  \mathcal{L}(\paramVec; \bm{y}^*) = \int \prod_{u = 1}^U f_{X_{u, 0}}\big(x_{u, 0};\, \paramVec\big)\prod_{n = 1}^{N_u} f_{X_{u, n}|X_{u, n-1}}\big(x_{u, n}|x_{u, n-1}; \, \paramVec\big)f_{Y_{u, n}|X_{u, n}}\big(y_{u, n}|x_{u, n}; \, \paramVec\big) dx_{1:U,0:N_u}.
  $$
  
  The parameter vector $\theta$ has shared components $\phi$, and unit specific components $\psi_{1:U}$.
  $$\theta = (\phi, \psi_{1:U})$$
  
\end{frame}

\begin{frame}{The problem}

  Independent models, why not do inference independently? 

  \begin{itemize}
    \item Each model may share features (or parameters), and we want to estimate using all of the data. 
  \end{itemize}
  Common inference procedures for single units rely on particle filters \citep{arulampalam02}.
  \begin{itemize}
    \item[{\color{green} $\checkmark$}] Particle filters can be applied independently to units.
    \item[{\color{red} $X$}] Iterated filtering (IF) is an extension used to perform maximum likelihood estimation \citep{ionides15}.
    \begin{itemize}
      \item IF introduces dependence because of shared $\theta$, making it a high-dimensional problem.
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Background: Data cloning and iterated filtering}
  
  IF is a special type of Data cloning \citep{lele07}. 
  
  Denote $\pi_i(\theta)$ as the posterior distribution of the parameter vector $\theta$ after the $i$th Bayesian update, and $\mathcal{L}(\paramVec; \bm{y}^*)$ as the likelihood
\begin{align*}
\pi_1(\theta) &\propto \mathcal{L}(\paramVec; \bm{y}^*)\pi_0(\theta), \\
\pi_2(\theta) &\propto \mathcal{L}(\paramVec; \bm{y}^*)\pi_1(\theta) \propto \mathcal{L}(\paramVec; \bm{y}^*)^2\pi_0(\theta),\\
&\quad\quad\vdots\\
\pi_m(\theta) &\propto \mathcal{L}(\paramVec; \bm{y}^*)^m\pi_0(\theta).
\end{align*}
If we let $m\rightarrow \infty$, the effect of the initial prior distribution diminishes, and the $m$th posterior has all of its mass centered at the MLE.

\end{frame}

\begin{frame}{Iterated filtering}
\setbeamercovered{transparent}
  Loosely speaking, iterated filtering is just data cloning with the additional pieces: 
  \begin{enumerate}
    \item Likelihood is approximated using sequential Monte Carlo. 
    \item Parameter particles are perturbed over time, treated like states.
    % \item Parameter particles reweighted using conditional log-likelihoods.
  \end{enumerate}\pause
  {\color{green} $\checkmark$} The perturbation of parameters is necessary to avoid particle depletion, a known problem with particle filters + Bayesian inference \citep{chen24}.
  
  {\color{red} $X$} The perturbations introduce a loss of information \citep{liu01}, so are decreased over the cloning iteration.
\end{frame}

\begin{frame}[plain, t]{Iterated Filtering for Panel Models}
\begin{columns}[t]
\begin{column}{0.69\textwidth}
  \begin{algorithm}[H]
  \notsotiny
\For{$m \in 1:M$}{
  Set $\Theta_{0, j}^{F, m} = \Theta_{j}^{m-1} = (\Phi_j^{m-1}, \Psi_{1:U, j}^{m-1})$ for $j \in \seq{1}{J}$\;
    \For{$u \in \seq{1}{U}$}{
        Set $\Theta_{u, 0, j}^{F, m} = (\Phi_{u, 0, j}^{F, m}, \Psi_{1:U, 0, j}^{F, m}) \sim h_{u, 0}\big(\cdot | \Theta^{F, m}_{u-1, j}; \sigma_{u, m}\big)$ \label{line:startu}\; 
      Initialize $X_{u, 0, j}^{F, m} \sim f_{X_{u, 0}}(x_{u, 0}; \Phi_{u, 0, j}^{F, m}, \Psi_{u, 0, j}^{F, m})$ for $j \in \seq{1}{J}$\;
        \For{$n \in \seq{1}{N_u}$} {
          Set $\Theta_{u, n, j}^{P, m} = (\Phi_{u, n, j}^{P, m}, \Psi^{P, m}_{1:U, n, j}) \sim h_{u, n}\big(\cdot |\Theta_{u, n-1, j}^{F, m}; \sigma_{u, m}\big)$ for $j \in \seq{1}{J}$ \label{line:perturbations}\;
          $X_{u, n, j}^{P, m} \sim f_{X_{u, n}|X_{u, n-1}}\big(x_{u, n}|X_{u, n-1, j}^{F, m}; \Phi_{u, n, j}^{P, m}, \Psi_{u, n, j}^{P, m}\big)$ for $j \in \seq{1}{J}$ \label{line:Xpred}\;
          $w_{u, n, j}^m = f_{Y_{u, n}|X_{u, n}}\big(y_{u, n}^*|X_{u, n, j}^{P, m};\Phi_{u, n, j}^{P, m}, \Psi_{u, n, j}^{P, m}\big)$ for $j \in \seq{1}{J}$ \label{line:weights}\;
          Draw $k_{\seq{1}{j}}$ with $P(k_j = i) = w_{u, n, i}^m / \sum_{v = 1}^J w_{u, n, v}^m$ for $i, j \in \seq{1}{J}$\;
          Set $X_{u, n, j}^{F, m} = X_{u, n, k_j}^{P, m}$, and $\big(\Phi_{u, n, j}^{F, m}, \Psi_{u, n, j}^{F, m}\big) = \big(\Phi_{u, n, k_j}^{P, m}, \Psi_{u, n, k_j}^{P, m}\big)$ for $j \in \seq{1}{J}$\;
            \uIf{$\mathrm{MARGINALIZE}$}{$\Psi^{F,m}_{\tilde u,n,j} = \Psi^{P,m}_{\tilde u,n,j}$ for all $\tilde u \neq u$, $j=\seq{1}{J}$\label{mpif:update}} 
       \Else{$\Psi^{F,m}_{\tilde u,n,j} = \Psi^{P,m}_{\tilde u,n,k_j}$ for all $\tilde u \neq u$, $j=\seq{1}{J}$}
        }
      Set $\Theta_{u, j}^{F, m} = \big(\Phi_{u, N_u, j}^{F, m}, \Psi_{u, N_{1:U}, j}^{F, m}\big)$ for $j \in \seq{1}{J}$ \label{line:endu}\;
    }
  Set $\Theta_j^{(m)} = \Theta_{U, j}^{F, m}$ for $j \in \seq{1}{J}$\;
}
\end{algorithm}
\end{column}

\begin{column}{0.35\textwidth}
\vspace{-3.2cm}

\textbf{\underline{Function Inputs:}}

\vspace{2mm}

  \begin{itemize}
    \footnotesize
    \item Initializer: $f_{X_{u, 0}}(x_{u, 0}; \, \theta)$.
    \item Process Simulator: $f_{X_{u, n}|X_{u, n-1}}(x_{u, n}|x_{u, n-1}; \, \theta)$.
    \item Measurement Model: $f_{Y_{u, n}|X_{u, n}}(y_{u, n}|x_{u, n} ; \, \theta)$. 
    \item Data $y_{u, n}^*$. 
    \item Iterations, Particles: $(M, J)$.
    \item Starting parameter swarm, $\Theta_j^0 = \big(\Phi_j^0, \Psi_{1:U, j}^0\big)$.
    \item Perturbation simulator: $h_{u, n}(\cdot | \varphi; \sigma)$.
    \item Variance sequence: $\sigma_{1:U, 1:M}$.
  \end{itemize}
  
\end{column}

\end{columns}
\end{frame}

\begin{frame}{PIF Theory}
  The panel iterated filter (PIF) is a type of IF, mitigating information loss \citep{breto20}. 
  It has been successfully used to estimate the MLE previously \citep[e.g.,][]{domeyer22}.
  
  \begin{theorem}[Chapter 4, Theorem 1]
    Extends theory of \citet{chen24} to panel models. Denote the output of the PIF algorithm as $\Theta_{1:J}^{(M)}$.
  Then there exists some positive sequences $\{C_M\}_{M \geq 1}$ and $\{\epsilon_M\}_{M \geq 1}$ where $\lim_{M \rightarrow \infty}\epsilon_M = 0$ such that for all $(J, M) \in \mathbb{N}^2$,
  $$
  E\bigg[\Big|\frac{1}{J}\sum_{i=1}^J \Theta_j^{(M)} - \hat{\theta}\Big|_2\bigg] \leq \frac{C_M}{\sqrt{J}} + \epsilon_M
  $$
  \end{theorem}
  
\end{frame}

\begin{frame}{Proof sketch and discussion}
  Conditions:
  \begin{itemize}
    \item On parameter space $\Theta$: Compact, corners not too ``sharp" \citep[regular compact set, Def~1 of][]{chen24}.
    \item Regularity conditions on the model (positive and finite likelihood, densities are smooth functions of $\theta$). 
    \item Conditions on the parameter perturbations (type of perturbations, and cooling schedule). 
  \end{itemize}
  Proof Sketch: 
  \begin{itemize}
    \item \citet{chen24} provides theory for convergence of IF of POMP models; we write a general PanelPOMP as a POMP model, and PIF is a special case of IF for these models. 
  \end{itemize}
\end{frame}

\begin{frame}{Iterated Filtering for panel models: a tradeoff}
  Iterated filtering is done one observation at a time. 
  In the panel setting, this means we also process one unit $u$ at a time.
  
  Ignoring perturbations, we have do \emph{unit} data cloning, iterating over $(m, u)$: 
  \begin{align}
    \pi_{m, u}(\theta) &\propto \mathcal{L}(\paramVec; y_u^*)\, \pi_{m, u-1}(\theta) = \mathcal{L}_u(\phi, \psi_u; y_u^*)\, \pi_{m, u-1}(\theta), \label{eq:PIFupdate}
  \end{align}
    Using $\pi_{0, 0}(\theta)$ as the initial prior distribution. Parameter dependence in posteriors introduced by iterating Eq.~\ref{eq:PIFupdate} over $u$.
    
    Two options of iterated filtering: 
    \begin{itemize}
      \item Perturb all parameters at each time step (IF2, high loss of information). 
      \item When using data from unit $u$, only perturb $\phi$ and $\psi_u$ (PIF, high particle depletion). 
    \end{itemize}
\end{frame}

\begin{frame}{MPIF motivation}
\setbeamercovered{transparent}
  If the previous prior $\pi_{m, u-1}(\theta)$ has parameter independence: $\pi_{m, u-1}(\theta) = f(\psi_{-u})g(\phi, \psi_{u})$, then there is no need to resample particles $\psi_{-u}$. 
  
  This would avoid the particle depletion \emph{and} loss-of-information, and motivates marginalized data cloning (repeating Eqs.~\ref{eq:margBayes}--\ref{eq:MPIFupdate}).
  
  \begin{align}
\tilde{\pi}_{m, u}(\theta) &\propto \mathcal{L}_{u}(y^*_u;\, \phi, \psi_u)\, \pi_{m, u-1}(\theta) \label{eq:margBayes}\\
\pi_{m, u}(\theta) &\propto \int \! \tilde{\pi}_{m, u}(\theta) \, d\phi \, d\psi_u \, \times \int \! \tilde{\pi}_{m, u}(\theta) \, d\psi_{-u} \label{eq:MPIFupdate}.
\end{align}\pause

Marginalization can happen in various ways. Next slide is a figure with bivariate Gaussian densities, marginalized over all parameters. 

\end{frame}

\begin{frame}{Marginalized Bayes: Guassian figure}

<<figDC, fig.height=3.25, fig.width=5>>=
Lambda <- rbind(
  c(0.8, 0.45),
  c(0.45, 1.1)
)

Prior <- list(
  'mean' = c(11, 8),
  'precision' = diag(c(1, 1))
)

all_DC_dists <- GG_DC(
  Lambda = Lambda,
  Prior = Prior,
  M = 100
)

all_MDC_dists <- GG_MDC(
  Lambda = Lambda,
  Prior = Prior,
  M = 100
)

M <- 5
K <- ncol(Lambda) - 1

df_mu_DC <- data.frame(
  'mu1' = rep(NA_real_, (M + 1) * K),
  'mu2' = rep(NA_real_, (M + 1) * K),
  'step' = rep(0:M, each = K),
  'group' = "DC"
)
df_Sigma_DC <- data.frame(
  'X1' = rep(NA_real_, (M + 1) * 101 * K),
  'X2' = rep(NA_real_, (M + 1) * 101 * K),
  'group' = 'DC',
  'step' = rep(NA_integer_, (M + 1) * 101 * K)
)


df_mu_MDC <- data.frame(
  'mu1' = rep(NA_real_, (M + 1) * K),
  'mu2' = rep(NA_real_, (M + 1) * K),
  'step' = rep(0:M, each = K),
  'group' = "MDC"
)
df_Sigma_MDC <- data.frame(
  'X1' = rep(NA_real_, (M + 1) * 101 * K),
  'X2' = rep(NA_real_, (M + 1) * 101 * K),
  'group' = "MDC",
  'step' = rep(NA_integer_, (M + 1) * 101 * K)
)

# for (j in 1:M) {
for (j in 0:M) {
  start_row <- (K * (j)) + 1
  start_sigma <- ((j) * 101 * K) + 1

  tmp_transition <- t(all_DC_dists[[j + 1]]$mean)
  df_mu_DC[start_row:(start_row + K - 1), c('mu1', 'mu2')] <- tmp_transition
  Sigma_j <- list(
    solve(all_DC_dists[[j + 1]]$precision)
  )
  df_Sigma_DC[start_sigma:(start_sigma + (101 * K) - 1),
              c('X1', "X2", "group")] <- calculate_ellipse(tmp_transition, Sigma_j)
  df_Sigma_DC[start_sigma:(start_sigma + (101 * K) - 1),
              'step'] <- as.integer(j)


  tmp_transition <- t(all_MDC_dists[[j + 1]]$mean)
  df_mu_MDC[start_row:(start_row + K - 1), c('mu1', 'mu2')] <- tmp_transition
  Sigma_j <- list(
    solve(all_MDC_dists[[j + 1]]$precision)
  )
  df_Sigma_MDC[start_sigma:(start_sigma + (101 * K) - 1),
              c('X1', "X2", "group")] <- calculate_ellipse(tmp_transition, Sigma_j)
  df_Sigma_MDC[start_sigma:(start_sigma + (101 * K) - 1),
              'step'] <- as.integer(j)
}

df_mu_DC$group <- 'DC'
df_mu_MDC$group <- 'MDC'
df_Sigma_DC$group <- 'DC'
df_Sigma_MDC$group <- 'MDC'

like_elipse <- calculate_ellipse(
  matrix(c(0, 0), nrow = 1),
  list(solve(Lambda))
) |>
  dplyr::select(-group)

df_all_mu <- dplyr::bind_rows(
  df_mu_DC, df_mu_MDC
)

df_all_Sigma <- dplyr::bind_rows(
  df_Sigma_DC, df_Sigma_MDC
)

method_names <- c(
  DC = 'Data Cloning',
  MDC = 'Marginalized Data Cloning'
)

ggplot(df_all_mu) +
  geom_point(
    aes(x = mu1, y = mu2, col = as.factor(step)),
    pch = 18, size = 1
  ) +
  geom_path(
    data = df_all_Sigma,
    aes(x = X1, y = X2, col = as.factor(step))
  ) +
  geom_path(
    data = like_elipse,
    aes(x = X1, y = X2), linetype = 'dashed'
  ) +
  geom_point(x = 0, y = 0, col = 'red', pch = 'x') +
  theme_bw() +
  scale_x_continuous(limits = c(-3.12, 13.5)) +
  scale_y_continuous(limits = c(-3.12, 13.5)) +
  theme(aspect.ratio = 1) +
  labs(color = 'Step') +
  scale_color_manual(
    values = c(
      "#800026",
      "#bd0026",
      "#e31a1c",
      "#fc4e2a",
      "#fd8d3c",
      "#feb24c"
    )
  ) +
  labs(x = TeX('$\\mu_2$'), y = TeX('$\\mu_1$')) +
  theme(legend.position = 'right') +
  facet_wrap(~group, labeller = labeller(group = method_names)) + 
  theme(plot.margin = unit(c(0, 0, 0, 0), 'cm'))
@

\end{frame}

\begin{frame}{MPIF theory}

  Just as IF extends data cloning (perturbations + particle approximation), the MPIF algorithm extends marginalized data cloning. 

  \begin{itemize}
    \item Existing theory for IF algorithms rely heavily on the data cloning principle \cite{ionides15, chen24}.
    \item The non-linearity introduced by the marginalization step invalidates these approaches.
    % \item Existing theory for IF algorithms cannot readily be extended to MPIF, because of the non-linearity introduced by the marginalization step. 
    \item A natural first question is whether or not marginalized data cloning converges.
    \begin{itemize}
      \item Unfortunately, a few toy examples suggests not always (computationally and analytically).
    \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}{Marginalized data cloning: Gaussian likelihoods}
  Convergence is explored via Gaussian likelihoods.
  
  The properties of this special case is relevant to the broader class of models that is well approximated by Gaussian models, (e.g., local asymptotic normality \citep{lecam00}).
  
  \begin{theorem}[Chapter 4, Theorem 2]
    Let $\mathcal{L}_u(y_u^*; \, \phi, \psi_u)$ be the likelihood that corresponds to a Gaussian distribution with mean $(\phi^*, \psi_u^*)$ and precision $\Lambda_u^* \in \R^{2\times2}$. Under suitable conditions on the matrices $\Lambda^*_u$, then if the initial prior density is Gaussian, then the density of the $m$th iteration of Eq.~\ref{eq:MPIFupdate} converges to a point mass at the MLE $(\phi^*, \psi_1^*, \ldots, \psi^*_U)$ as $m\rightarrow \infty$.
  \end{theorem}
\end{frame}

\begin{frame}{Gaussian Theory: Proof sketch}

\begin{itemize}
  \item Gaussian priors + Gaussian likelihoods imply Gaussian posteriors.
  \item Transform data so likelihood is centered at zero.
  \item The marginalization step only modifies the covariance, setting off-diagonal terms to zero. Conditions ensure this loss of information is not too large.
  \item Diagonals of covariance shrink to zero asymptotically at rate $1/m$.
  \item Each unit-iteration updates the $\phi$ and $\psi_u$ components of $\mu_m$.
  \item $\mu_m = \big(\prod_{i = 1}^m\prod_{u = 1}^U A_{u, i}\big) \mu_0 = \big(\prod_{i = 1}^m P_m\big) \mu_0$, with $\|P_m\|_2 = 1 - \epsilon_m/m + o(1/m)$, with $\epsilon_m$ positive, bounded.
\end{itemize}

\end{frame}

\begin{frame}{Convergence with perturbations}

    \begin{theorem}[Chapter 4, Corollary 1]
    Consider the same setup as Theorem~2 (Chapter~4). If parameters are perturbed prior performing the Bayes update at each step using Gaussian additive noise with mean $0$ and covariance $\sigma^2_m\Sigma_0$, then if $\sigma_m^2 = o(1/m)$, the algorithm still converges to the MLE as $m\rightarrow \infty$. 
  \end{theorem}
  
  \begin{itemize}
    \item Useful heuristic: a more dispersed prior typically results in a posterior distribution closer to the likelihood function.
    \item Adding Gaussian noise at each step results in larger updates towards to MLE at each step.
    \item Heuristically, convergence of unperturbed case implies convergence of perturbed case, if perturbation variance decreases to zero.
  \end{itemize}

\end{frame}

\begin{frame}{The MPIF advantage: improved particle representation}

  One perspective is that the marginalization adds a small amount of bias in each intermediate posterior distribution.
  
  \begin{itemize}
    \item Theorem~2 (Chapter 4) gives formal conditions where the bias at each step is small, and algorithm converges to MLE (no bias).
    \item The advantage is improved particle representations. In this case, MPIF may still be preferable even if there is small bias.
  \end{itemize}
  
  Thus, we can think of MPIF as improving a bias-variance tradeoff.
  
  \begin{itemize}
    \item To demonstrate this, perform a single unit-iteration of PIF and MPIF on Gaussian data and model.
  \end{itemize}
  
\end{frame}

\begin{frame}{Bias-Variance tradeoff}
  
<<fig.height=4.5, fig.width=5, out.width='0.65\\maxwidth'>>=
set.seed(54321)
N <- 100  # Number of observations  
J <- 1000  # Number of particles

like_mean <- 0  # Mean of the likelihood function
like_var <- 1  # Variance of the likelihood function
prior_mean <- c(2, 2)  # mean of the prior distribution
prior_cov <- rbind(c(2, 0.5), c(0.5, 1))  # Covariance matrix of the prior

Likelihood_Yi <- function(y_i, theta) {
  dnorm(y_i, mean = theta)
}

# Simulate Data from likelihood model
data_y <- rnorm(N, mean = like_mean, sd = sqrt(like_var))

# Get particle representation of prior
prior <- MASS::mvrnorm(
  J,
  mu = prior_mean, 
  Sigma = prior_cov
)

# Marginals.
prior1 <- prior[, 1]
prior2 <- prior[, 2]

# Keep track of how particle marginals change over time
all_priors1 <- matrix(nrow = N + 1, ncol = J)
all_priors2 <- matrix(nrow = N + 1, ncol = J)

all_priors1[1, ] <- prior1
all_priors2[1, ] <- prior2

# Update particle distribution, one observation at a time. 
for (i in 1:N) {
  
  prior1 <- rnorm(n = length(prior1), mean = prior1, sd = 0.02)  # small random noise
  weights <- Likelihood_Yi(data_y[i], theta = prior1)
  reg_weights <- weights / sum(weights)
  prior1 <- prior1[sample(1:length(prior1), length(prior1), prob = reg_weights, replace = TRUE)]
  prior2 <- prior2[sample(1:length(prior1), length(prior1), prob = reg_weights, replace = TRUE)]
  
  # Keep track of new particle distributions. 
  all_priors1[i+1, ] <- prior1
  all_priors2[i+1, ] <- prior2
}

# Convert to data.frame, for plotting. 
all_priors1_df <- as.data.frame(all_priors1) %>% 
  mutate(iteration = 0:N) %>% 
  pivot_longer(-iteration, names_to = 'sample', values_to = 'val') %>% 
  mutate(dist = '1')

all_priors2_df <- as.data.frame(all_priors2) %>% 
  mutate(iteration = 0:N) %>% 
  pivot_longer(-iteration, names_to = 'sample', values_to = 'val') %>% 
  mutate(dist = '2')

all_priors <- dplyr::bind_rows(all_priors1_df, all_priors2_df)

tmp <- all_priors %>% 
  group_by(dist, iteration) %>% 
  distinct(val) %>% ungroup()

compute_posterior <- function(y, sigma2, mu0, Sigma0) {
  # Ensure inputs are appropriate structures
  if (!is.vector(y) || !is.numeric(y)) stop("y must be a numeric vector")
  if (!is.matrix(Sigma0) || nrow(Sigma0) != 2 || ncol(Sigma0) != 2) stop("Sigma0 must be a 2x2 matrix")
  if (length(mu0) != 2) stop("mu0 must be a vector of length 2")
  
  # Calculate some needed constants
  N <- length(y)
  y_sum <- sum(y)
  
  # Compute the inverse of Sigma0
  Sigma0_inv <- solve(Sigma0)
  
  # Define the precision matrix of the likelihood part and prior part
  likelihood_precision <- diag(c(N / sigma2, 0))
  
  # Compute the posterior precision matrix
  posterior_precision <- likelihood_precision + Sigma0_inv
  
  # Compute the posterior covariance matrix (inverse of the precision matrix)
  Sigma_n <- solve(posterior_precision)
  
  # Compute the posterior mean
  b <- Sigma0_inv %*% mu0 + c(y_sum / sigma2, 0)
  mu_n <- Sigma_n %*% b
  
  # Return a list containing the posterior mean and covariance matrix
  list(mu_n = mu_n, Sigma_n = Sigma_n)
}

true_post <- compute_posterior(data_y, like_var, mu0 = prior_mean, Sigma0 = prior_cov)

post_df <- data.frame(
  dist = c('1', '2', '3'),
  mean = c(true_post$mu_n[1], true_post$mu_n[2], true_post$mu_n[2]),
  sd = c(sqrt(true_post$Sigma_n[1, 1]), sqrt(true_post$Sigma_n[2, 2]), sqrt(true_post$Sigma_n[2, 2]))
)

depParticlesPIF <- tmp %>% 
  group_by(dist, iteration) %>% 
  summarize(n = n_distinct(val)) %>% 
  mutate(dist = factor(dist, levels = c('1', '2'), labels = c(TeX('$\\Psi_1$'), TeX('$\\Psi_2$'))))

depParticlesMPIF <- depParticlesPIF %>% 
  mutate(
    n = case_when(dist == 'Psi[1]' ~ n, TRUE ~ max(n))
  )

depletion_1 <- depParticlesPIF %>% 
  ggplot(aes(x = iteration, y = n)) + 
  facet_wrap(
    ~dist, nrow = 1, labeller = label_parsed
  ) + 
  geom_line(col = MARG_COLS[2]) +
  geom_line(data = depParticlesMPIF, col = MARG_COLS[1], linetype = 'dashed') + 
  theme_bw() + 
  ylab("Number of\nUnique Particles") + 
  xlab(TeX("Observation Number $(n \\in 1 : N_1)$"))

final_prior <- data.frame(theta1 = prior1, theta2 = prior2)

posterior_ellipse <- calculate_ellipse(t(true_post$mu_n), list(true_post$Sigma_n))

final_prior$theta2_init <- all_priors2[1, ]

final_prior_long <- pivot_longer(
  data = final_prior, 
  cols = c("theta2", "theta2_init"),
  names_to = 'type',
  values_to = 'theta2'
)

depletion_2 <- ggplot() + 
  geom_point(data = final_prior_long, aes(x = theta1, y = theta2, col = type)) +
  geom_path(data = posterior_ellipse, aes(x = X1, y = X2), linetype = 'dashed', col = 'black', linewidth = 1) + 
  facet_wrap(~type, labeller = as_labeller(c(theta2 = "Unmarginalized", theta2_init = "Marginalized"))) + 
  theme_bw() + 
  ylab(TeX("$\\Psi_2$")) + 
  xlab(TeX("$\\Psi_1$")) + 
  theme(axis.title.y = element_text(margin = unit(c(0, 0.2, 0, 0.7), 'cm'))) + 
  scale_color_manual(values = rev(MARG_COLS)) + 
  theme(legend.position = 'none')

cowplot::plot_grid(depletion_1, depletion_2, nrow = 2, labels = "AUTO", rel_heights = c(0.425, 0.575))
@
  
\end{frame}

\begin{frame}{Gompertz population model: high-dimensional space}

The MPIF algorithm is demonstrated on data simulated from a stochastic population model. We have $U$ ranging from $5--2500$, and $N_u = N \in \{20, 50, 100\}$.
\begin{itemize}
  \item $X_{u, n+1} = K^{1- \exp{r_u}}_u \epsilon_{u, n}$, $\epsilon_{u, n} \sim N(0, \sigma^2_u)$. 
  \item $Y_{u, n} | X_{u, n} \sim N\big(\log X_{u, n}, \tau^2_u\big)$
  \item Fix $K_u = 1$, $X_{u, 0} = 1$. Estimate $\sigma_u^2 = \sigma^2 = 0.01$, $r_u = r = 0.1$, and $\tau_u^2 = 0.01$ for all $u$.  
\end{itemize}

This is log-linear Gaussian, so exact likelihood can be computed \citep{kalman60}. Parameter choices match \citet{breto20}. 

\end{frame}

\begin{frame}
<<loadGompertz, echo=FALSE, include=FALSE>>=
results0 <- readRDS("data/GompertzRL3.rds")
results1 <- readRDS("data/GompertzBigUnitRL3.rds")
results2 <- readRDS("data/GompertzHugeRL3.rds")

resultsBig <- dplyr::bind_rows(results1, results2) %>% 
  filter(N == 50)

KF <- results0 %>%
  filter(algorithm == "KF")

IF <- results0 %>%
  filter(algorithm == "IF")

fct_orders <- paste(
  rep(c("U = 5", "U = 15", "U = 50", "U = 100"), each = 4),
  rep(c("N = 20", "N = 50", "N = 100", "N = 200"))
)

IF %>%
  pivot_longer(
    cols = starts_with("ll_"),
    names_to = "iter",
    values_to = "loglik",
    names_prefix = "ll_"
  ) %>%
  mutate(
    n_obs = paste0("N = ", N),
    n_units = paste0("U = ", U)
  ) -> IF_long

KF_max <- KF %>%
  filter(N == 50) %>% 
  mutate(
    n_obs = paste0("N = ", N),
    n_units = paste0("U = ", U)
  ) %>%
  select(-COOLING_TYPE, -J, -M, -BLOCK, -COOLING, -(ll_2:ll_50)) %>%
  rename(ll_max = ll_1) %>%
  group_by(n_units, seed = data_seed) %>%
  summarize(
    ll_max = max(ll_max),
    ll_min = min(ll_max)
  ) %>% 
  mutate(n_units = factor(n_units, levels = c('U = 5', 'U = 15', 'U = 50', 'U = 100')))


fct_orders <- paste(
  rep(c("U = 200", "U = 500", "U = 1000", "U = 2500"))
)

resultsBig %>%
  pivot_longer(
    cols = starts_with("ll_"),
    names_to = "iter",
    values_to = "loglik",
    names_prefix = "ll_"
  ) %>%
  mutate(
    n_obs = paste0("N = ", N),
    n_units = paste0("U = ", U),
    n_units = factor(n_units, levels = fct_orders)
  ) -> IF_longBig

IF_long_ntruth <- IF_longBig %>% 
  filter(iter != 'truth') %>% 
  mutate(iter = as.numeric(iter))

IF_truth <- IF_longBig %>% 
  filter(iter == 'truth') %>% 
  select(-iter) %>% 
  mutate(
    n_units = factor(n_units, levels = fct_orders)
  ) %>%
  group_by(n_units) %>% 
  distinct(loglik)
@

<<gompFig1, fig.height=4>>=
gg_smallU <- IF_long %>%
  filter(J == 1000) %>%
  filter(N == 50) %>%
  group_by(iter, BLOCK, n_units) %>%
  summarize(
    min = quantile(loglik, 0.10),
    # min = min(loglik), 
    median = median(loglik),
    logmeanexp = logmeanexp(loglik),
    mean = mean(loglik),
    max = max(loglik)
  ) %>%
  mutate(
    iter = as.numeric(iter),
    BLOCK = factor(BLOCK, levels = c(TRUE, FALSE)),
    n_units = factor(n_units, levels = c('U = 5', 'U = 15', 'U = 50', 'U = 100'))
  ) %>%
  filter(iter > 0) %>%
  ggplot(aes(x = iter, col = BLOCK)) +
  geom_line(aes(y = max)) +
  geom_segment(aes(y = min, yend = max, xend = iter)) +
  facet_wrap(~n_units, scales = 'free', nrow = 1) +
  theme_bw() +
  geom_hline(data = KF_max, aes(yintercept = ll_max), linetype = 'solid') +
  ylab("log-likelihood") +
  theme(axis.text = element_text(size = 6), legend.position = 'none', axis.title.x = element_blank()) +
  scale_color_manual(name = 'Marginalized', values = MARG_COLS)

gg_bigU <- IF_long_ntruth %>%
  filter(J == 1000) %>%
  filter(N == 50) %>% 
  mutate(n_units = factor(n_units, levels = fct_orders)) %>%
  group_by(iter, BLOCK, n_units) %>%
  summarize(
    min = quantile(loglik, 0.10),
    median = median(loglik),
    logmeanexp = logmeanexp(loglik),
    mean = mean(loglik),
    max = max(loglik)
  ) %>%
  mutate(
    BLOCK = factor(BLOCK, levels = c(TRUE, FALSE))
  ) %>%
  filter(iter > 0) %>%
  ggplot(aes(x = iter, col = BLOCK)) +
  geom_line(aes(y = max)) +
  geom_segment(aes(y = min, yend = max, xend = iter)) +
  facet_wrap(~n_units, scales = 'free', nrow = 1) +
  theme_bw() +
  geom_hline(data = IF_truth, aes(yintercept = loglik), linetype = 'dashed') +
  ylab("log-likelihood") +
  xlab("Iteration number (m)") +
  theme(axis.text = element_text(size = 6), legend.position = 'bottom') +
  scale_color_manual(name = 'Marginalized', values = MARG_COLS)

cowplot::plot_grid(gg_smallU, gg_bigU, nrow = 2, rel_heights = c(0.4, 0.6))
@
\end{frame}

\begin{frame}{Discussion}
  \begin{itemize}
    \item In all tested models, the MPIF algorithm outperforms PIF (especially as $U$ and $d_\psi$ grow).
    \item Even poor performing replicates of MPIF often outperform PIF, despite weaker theoretical guarantees.
    \item Improved performance is a result of reduced particle depletion.
    \item Stronger theory for MPIF is available in some special cases, each covered by Theorem~1 of Chapter~4:
    \begin{itemize}
      \item No shared parameters: equivalent to performing IF2 independently to each model. 
      \item No unit-specific parameters: MPIF is the same as PIF.
    \end{itemize}
  \end{itemize}
\end{frame}

\end{document}
%EoF
