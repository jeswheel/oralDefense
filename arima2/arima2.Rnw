\documentclass[aspectratio=169]{beamer}
\usetheme{gotham}

	\usepackage{standalone}
	\usepackage{tikz}
	\usepackage{pgfplots}
	\usepackage{tabularray} % Typeset tabulars and arrays (contains equivalent of longtable, booktabs and dcolumn at least)
		\UseTblrLibrary{booktabs} % to load extra commands from booktabs
	\usepackage{changepage}
	\usepackage{minted}
		\definecolor{codeback}{rgb}{0.90,0.91,0.92}
		\definecolor{codebackdark}{rgb}{0.10,0.11,0.12}

	\newcommand{\famName}[1]{\textsc{#1}}
	\newcommand{\themename}{\textbf{\textsc{Gotham}}}

<<arima2Setup>>=
library(arima2)
library(tidyverse)
library(latex2exp)
root <- ""
@


\begin{document}

\section{Likelihood Maximization for ARMA models}

\begin{frame}{Auto-regressive moving average (ARMA) models}
  ARMA models are the most frequently used approach to modeling time series data.
  ARMA models are as foundational to time series analysis as linear models are to regression analysis, and they are often used in conjunction for regression with ARMA errors.
  
  \begin{block}{ARMA model definition}
    A time series $Y_{1:N}$ is called ARMA$(p, q)$ if it is (weakly) stationary and
    \begin{equation}
    Y_n = \phi_1Y_{n - 1} + \cdots + \phi_pY_{n - p} + w_n + \MApar_1w_{n-1} + \ldots + \MApar_qw_{n - q},\label{eq:arma}
    \end{equation}
    with $\{w_n; n = 0, \pm1, \pm2, \ldots\}$ denoting a mean zero white noise (WN) processes with variance $\sigma_w^2 > 0$, and $\phi_p \neq 0$, $\MApar_q \neq 0$.
  \end{block}
  
  We refer to the positive integers $p$ and $q$ of Eq.~\myeqref{eq:arma} as the autoregressive (AR) and moving average (MA) orders, respectively.
\end{frame}

\begin{frame}[allowframebreaks]{State space formulation}
  For practitioners, ARMA models do not appear to be SSMs.
  However, inference methodology treats ARMA models as \emph{non-mechanistic} SMMs. Let $r = \max(p, q+1)$, and we now define
  \begin{equation*}
    X_n = \begin{pmatrix}
  Y_n \\
  \phi_2 Y_{n - 1} + \ldots + \phi_r Y_{n - r + 1} + \MApar_{1}w_{n} + \ldots + \MApar_{r - 1}w_{n - r + 2} \\
    \phi_3 Y_{n - 1} + \ldots + \phi_r Y_{n - r + 2} + \MApar_{2}w_{n} + \ldots + \MApar_{r - 1}w_{n - r + 3} \\
    \vdots \\
    \phi_r Y_{n - 1} + \MApar_{r - 1}w_n
  \end{pmatrix} \in \R^r
  \end{equation*}
  \begin{equation*}
  T = \begin{pmatrix}
\phi_1 & 1 & 0 & \ldots & 0 \\
\phi_2 & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & & \ddots & \\
\phi_{r-1} & 0 & \ldots &  & 1 \\
\phi_r & 0 & \ldots & & 0
\end{pmatrix} \in \R^{r \times r}, \quad\quad
Q = \begin{pmatrix}
  1 \\ \MApar_1 \\ \vdots \\ \MApar_{r - 1}
\end{pmatrix} \in \R^r
  \end{equation*}

We can then recover the ARMA model using the following state space formulation:
\begin{align*}
  X_n &= TX_{n - 1} + Qw_n \\
  Y_n &= \begin{pmatrix} 1 & 0 & \ldots & 0\end{pmatrix} X_n
\end{align*}

This results in a linear-Gaussian SSM, and the likelihood function $\mathcal{L}(\paramVec)$ can be evaluated exactly using the Kalman filter \citep{kalman60}.
\begin{itemize}
  \item The likelihood can be maximized by combining this with a numeric optimizer \citep{gardner1980}.
\end{itemize}

This approach has been the standard method for fitting ARIMA models since the early 2000's due to modern computing capabilities \citep{ripley2002}. 

\end{frame}

\begin{frame}{Optimization Shortcomings}
This existing approach frequently results in sub-optimal parameter estimates.
To demonstrate this, we fit an ARMA$(2, 2)$ and an ARMA$(2, 1)$ model to data generated from an ARMA$(2, 2)$ model. 
The ARMA$(2, 1)$ is formally a special case of an ARMA$(2, 2)$ model, with $\MApar_2 = 0$. 

In \code{R}, we draw a single instance from Model class 2: $y^*_{1:100} \sim \mathrm{ARMA}(2, 2)$ with: 
\begin{itemize}
  \item $(\phi_1, \phi_2, \MApar_1, \MApar_2) = (0.2, -0.1, 0.4, 0.2)$
  \item $w_n \overset{\text{iid}}{\sim} N(0, 1)$. 
  \item Intercept $\mu = 13$ so that $E[Y_n] \neq 0$.
\end{itemize}

<<armaSeed, echo=FALSE,include=FALSE,comment=FALSE>>=
set.seed(839076)
y <- 13 + arima.sim(
  n = 100, model = list(ar = c(0.2, -0.1), ma = c(0.4, 0.2))
)
@

\end{frame}

\begin{frame}{Fitting ARMA models}
\setbeamercovered{transparent}

The \citet{gardner1980} is the standard method for fitting ARMA model parameters. It is implemented in the base \code{stats} package in R, as well as the \code{statsmodels} module in Python.

% Now we fit both both classes of models: $\mathrm{ARMA}(2, 1)$ and $\mathrm{ARMA}(2, 2)$: 
<<fitARMA, echo=TRUE>>=
mod1 <- stats::arima(y, order = c(2, 0, 1))

mod2 <- stats::arima(y, order = c(2, 0, 2))
@

\pause 
The likelihood of \code{mod1} is \Sexpr{round(mod1$loglik, 1)}, and the likelihood of \code{mod2} is \Sexpr{round(mod2$loglik, 1)}. The \alert{smaller} model has a log-likelihood that is \Sexpr{round(mod1$loglik - mod2$loglik, 1)} units \alert{higher} than the larger model, which is mathematically impossible under proper optimization. 
\end{frame}

\begin{frame}{Convergence to local optima}
  The difficulty is that the likelihood surface is often multimodal, and the existing procedure runs the risk of converging to a local solution \citep{ripley2002}.

<<MultiModeSetup, echo=FALSE>>=
sample_data <- function(data, job, n, P, Q, ...) {

  res <- list()

  coefs <- sample_ARMA_coef(
    order = c(P, Q), Mod_bounds = c(0.1, 0.9), min_inv_root_dist = 0.1
  ) |> round(digits = 2)

  res$ars <- coefs[grepl("^ar[[:digit:]]+", names(coefs))]
  res$mas <- coefs[grepl("^ma[[:digit:]]+", names(coefs))]

  res$x <- arima.sim(
    n = n,
    model = list(ma = res$mas)
  )

  res
}

checkImprove <- function(data, job, instance, ...) {

  arma2_fit <- tryCatch(
    arima2::arima(
      instance$x, order = c(0, 0, 1), SSinit = 'Rossignol2011', max_inv_root = 0.99,
      max_repeats = 10, include.mean = FALSE
    ),
    error = function(e) list(num_starts = 0, coef = c('ma1' = 0))
  )

  c('num_starts' = arma2_fit$num_starts, 'll_improve' = ifelse(arma2_fit$num_starts > 10, arma2_fit$all_values[arma2_fit$num_starts] - arma2_fit$all_values[1], NA_real_), "truth" = instance$mas, "est" = arma2_fit$coef)
}

get_profile <- function(model, npts) {
  results <- matrix(ncol = 2, nrow = npts)

  fit_arma <- function(fixed) {

    P <- model$arma[1]
    Q <- model$arma[2]

    arima2::arima(
      model$x, order = c(0, 0, 1), fixed = c(fixed), SSinit = 'Rossignol2011',
      max_iters = 1, include.mean = FALSE
    )$loglik
  }

  fixed_vals <- seq(-1+1e-8, 1-1e-8, length.out = npts)
  lls <- purrr::map_dbl(fixed_vals, fit_arma)
  results[, 1] <- fixed_vals
  # results[, 2] <- model$coef[2]
  results[, 2] <- lls
  colnames(results) <- c(names(model$coef), 'loglik')
  as.data.frame(results)
}

ma1_res <- readRDS(paste0(root, "data/sim_MA1_results.rds"))

ma1_improve <- ma1_res %>%
  filter(num_starts > 10) %>%
  filter(abs(est.ma1) < 0.95)

exp_seed <- 100000
prob_seed <- 50000

# Loop --------------------------------------------------------------------

job_ids <- c(10463, 16385, 22383, 41307)
ggs <- list()

for (j in 1:length(job_ids)) {
  i <- job_ids[j]
  # i <- test_mods[j]
  set.seed(prob_seed + (i %% 25000) - 1)

  instance <- sample_data(
    NULL,
    i,
    pull(ma1_res, n)[i],
    P = 0, Q = 1
  )

  set.seed(exp_seed + i)

  arma2_fit <- tryCatch(
    arima2::arima(
      instance$x, order = c(0, 0, 1), SSinit = 'Rossignol2011', max_inv_root = 0.99,
      max_repeats = 10, include.mean = FALSE
    ),
    error = function(e) list(num_starts = 0)
  )

  stats1 <- stats::arima(
    instance$x,
    order = c(0, 0, 1),
    SSinit = 'Rossignol2011',
    include.mean = FALSE
  )

  stats2 <- stats::arima(
    instance$x,
    order = c(0, 0, 1),
    SSinit = 'Rossignol2011',
    include.mean = FALSE,
    method = 'CSS'
  )

  mod_prof <- get_profile(arma2_fit, 1000)
  # mod_prof <- profile(arma2_fit, which = 1, lower = -1 + 1e-8, upper = 1 - 1e-8, npts = 1000)

  diffs <- case_when(
    j == 1 ~ 0.9,
    j == 2 ~ 3,
    j == 3 ~ 0.5,
    j == 4 ~ 0.5,
    TRUE ~ 2
  )

  df_lines <- data.frame(
    'value' = c(
      instance$mas,
      arma2_fit$coef[1],
      stats1$coef[1],
      stats2$coef[1]
    ),
    'type1' = c('truth', 'est', 'est', 'init'),
    'type2' = c('truth', 'est1', 'est2', 'truth')
  )
  
  df_lines <- df_lines %>% 
    filter(type2 != 'est1')

  gg_tmp <- mod_prof %>%
  filter(loglik > max(loglik) - diffs) %>%
  ggplot(aes(x = ma1, y = loglik)) +
  geom_line() +
  geom_vline(data = df_lines, aes(xintercept = value, col = type1, linetype = type1)) +
  # geom_vline(xintercept = instance$mas, col = 'black') +
  # geom_vline(xintercept = stats2$coef, col = 'black', linetype = 'dotted') +
  theme_bw() +
  labs(
    x = TeX('$\\varphi_1$'),
    y = "Loglikelihood"
  ) +
  theme(plot.title = element_text(hjust = 0.5)) +
  # Add manual legend using dummy data
  scale_linetype_manual(
    name = NULL,
    values = c(
      "truth" = "longdash",
      "est" = "solid",
      "init" = "dashed"
    ), labels = c("truth" = 'Truth', "est" = "Estimate", "init" = 'CSS Initialization')
  ) +
  scale_color_manual(
    name = NULL,
    values = setNames(c("red", "black", "blue"), c("truth", "est", 'init')),
    labels = c("truth" = 'Truth', "est" = "Estimate", "init" = 'CSS Initialization')
    # labels = setNames(c("arima2::arima", "stats::arima"), c("est1", "est2"))
  ) +
  # guides(
  #   linetype = guide_legend(order = 1),
  #   color = guide_legend(order = 2)
  # ) +
  theme(legend.title = element_blank(), legend.margin = margin(c(-2,0,0,0)),
        legend.key.spacing.y = unit(10, "pt"), legend.spacing.y = unit(35, 'pt'))

  if (j == 2 | j == 4) {
    gg_tmp <- gg_tmp + theme(axis.title.y = element_blank())
  }

  if (j == 1 | j == 2) {
    gg_tmp <- gg_tmp + theme(axis.title.x = element_blank())
  }

  ggs[[j]] <- gg_tmp
}
@
  

\begin{figure}[ht]
<<mutliMode-fig, fig.height=2.8, fig.width=6, echo=FALSE, out.width='0.9\\maxwidth'>>=
ggpubr::ggarrange(
  plotlist = ggs,
  legend = 'right',
  common.legend = TRUE
)
@
\caption{\label{fig:multiMode}The profile log-likelihood of data simulated from four distinct MA(1) models. The dashed, red line indicates the true value of $\MApar_1$; the short-dashed blue line is the CSS-initialization. The solid black line correspond to the estimate $\hat{\MApar}_1$ using \code{stats:arima}.}
\end{figure}

\end{frame}

\begin{frame}{Multiple parameter initializations}
\setbeamercovered{transparent}
  In other contexts with multi-model loss functions, the optimization is often repeated using multiple initializations. 
  However, I have seen \alert{no instances} of this for ARIMA models. There are a few challenges: 
  \begin{itemize}
    \item Most users don't know about the possibility of converging to local solutions. 
    \item Their are complex constraints of possible initialization. 
    \begin{itemize}
      \item Constraints are on the roots of polynomials formed by model parameters, not directly on parameters themselves.
    \end{itemize}
  \end{itemize}
  \pause
  The roots of the polynomials $\Phi(x) = 1 - \phi_1 x - \phi_2x^2 - \ldots - \phi_px^p$ and $\Psi(x) = 1 + \MApar_1 x + \MApar_2x^2 + \ldots + \MApar_qx^q$ must lie outside the complex unit circle.
\end{frame}

\begin{frame}
  For parameters to be real, the roots need to be sampled as real or conjugate pairs. We cannot sample all roots as conjugate pairs (or real), as this would result in specific parameters being all one sign. Our approach for each root is the following:
  \begin{itemize}
    \item Sample inverted-root magnitudes uniformly $U(\gamma, 1-\gamma)$.
    \item With probability $p = \sqrt{1/2}$, sample inverted-root pairs as real.
    \begin{itemize}
      \item If real, assign the same sign with probability $p$.
      \item If complex, sample angle from $U(0, \pi)$, and use to assign conjugate pairs of inverted-roots.
    \end{itemize}
    \item With roots sampled, calculate corresponding coefficients and perform optimization routine.
    \item Repeat until convergence.
  \end{itemize}
\end{frame}

\begin{frame}{Revisiting toy example}
\setbeamercovered{transparent}

Now we'll fit the exact same models using the \code{arima2} package:

<<ARMAsetSeed2, echo=FALSE>>=
set.seed(746168)
@

<<fitARMA2, echo=TRUE, >>=
mod1v2 <- arima2::arima(y, order = c(2, 0, 1))

mod2v2 <- arima2::arima(y, order = c(2, 0, 2))
@

With this new algorithm and software, the likelihood of \code{mod1v2} is \Sexpr{round(mod1v2$loglik, 1)}, and the likelihood of \code{mod2v2} is \Sexpr{round(mod2v2$loglik, 1)}.

\pause
The likelihood of the smaller model was unchanged, but the larger model had an increase in log-likelihood of \Sexpr{round(mod2v2$loglik - mod2$loglik, 1)}. The likelihoods of the nested models are now \alert{consistent}.
\end{frame}

\begin{frame}{Summary}
\setbeamercovered{transparent}
ARMA models are not necessarily state-of-the art statistical models. Why does this project matter?
  \begin{itemize}
    \item ARMA models are among the most frequently used approaches in all of statistics, so even small improvements are worth the effort.\pause
    \item Software that claims to maximize model likelihoods fails to do so in a large number of cases ($>20\%$).\pause
    \item ARMA models are often used in conjunction with linear regression. Likelihood ratio tests are common for testing the inclusion / significance of regression parameters.\pause
    \begin{itemize}
      \item Typical improvements in log-likelihood in the range $(0.22, 1.46)$. This shortcoming in one or both model is large enough to change the outcome of these tests.\pause
    \end{itemize}
    \item Even if outcomes are unchanged, confidence that software / algorithms will reliably do what you expect is important. 
  \end{itemize}
\end{frame}

\end{document}
%EoF
