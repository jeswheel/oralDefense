\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usetheme{gotham}

	\usepackage{standalone}
	\usepackage{tikz}
	\usepackage{pgfplots}
	\usepackage{tabularray} % Typeset tabulars and arrays (contains equivalent of longtable, booktabs and dcolumn at least)
		\UseTblrLibrary{booktabs} % to load extra commands from booktabs
	\usepackage{changepage}
	\usepackage{minted}
		\definecolor{codeback}{rgb}{0.90,0.91,0.92}
		\definecolor{codebackdark}{rgb}{0.10,0.11,0.12}

	\newcommand{\famName}[1]{\textsc{#1}}
	\newcommand{\themename}{\textbf{\textsc{Gotham}}}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hldef{(arima2)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# Attaching package: 'arima2'}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# The following object is masked from 'package:stats':\\\#\# \\\#\# \ \ \ \ arima}}\begin{alltt}
\hlkwd{library}\hldef{(tidyverse)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# -- Attaching core tidyverse packages ----------------------------------------------------------- tidyverse 2.0.0 --\\\#\# v dplyr \ \ \ \ 1.1.4 \ \ \ \ v readr \ \ \ \ 2.1.5\\\#\# v forcats \ \ 1.0.0 \ \ \ \ v stringr \ \ 1.5.1\\\#\# v ggplot2 \ \ 3.5.2 \ \ \ \ v tibble \ \ \ 3.2.1\\\#\# v lubridate 1.9.4 \ \ \ \ v tidyr \ \ \ \ 1.3.1\\\#\# v purrr \ \ \ \ 1.0.4}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# -- Conflicts ----------------------------------------------------------------------------- tidyverse\_conflicts() --\\\#\# x dplyr::filter() masks stats::filter()\\\#\# x dplyr::lag() \ \ \ masks stats::lag()\\\#\# i Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors}}\begin{alltt}
\hlkwd{library}\hldef{(latex2exp)}
\hldef{root} \hlkwb{<-} \hlsng{""}
\end{alltt}
\end{kframe}
\end{knitrout}

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\section{Likelihood Maximization for ARMA models}

\begin{frame}{Auto-regressive moving average (ARMA) models}
  ARMA models are the most frequently used approach to modeling time series data.
  % ARMA models are as foundational to time series analysis as linear models are to regression analysis, and they are often used in conjunction for regression with ARMA errors.
  
  \begin{block}{ARMA model definition}
    A time series $Y_{1:N}$ is called ARMA$(p, q)$ if it is (weakly) stationary and
    \begin{equation}
    Y_n = \phi_1Y_{n - 1} + \cdots + \phi_pY_{n - p} + w_n + \MApar_1w_{n-1} + \ldots + \MApar_qw_{n - q},\label{eq:arma}
    \end{equation}
    with $\{w_n; n = 0, \pm1, \pm2, \ldots\}$ denoting a mean zero white noise (WN) processes with variance $\sigma_w^2 > 0$, and $\phi_p \neq 0$, $\MApar_q \neq 0$.
  \end{block}
  
  $p$ and $q$ of Eq.~\myeqref{eq:arma} as the autoregressive (AR) and moving average (MA) orders, respectively.
\end{frame}

\begin{frame}[allowframebreaks]{State space formulation}
  How are they relevant? Inference methodology treats ARMA models as \emph{non-mechanistic} SMMs. Let $r = \max(p, q+1)$, and assume $w_n \overset{\mathrm{iid}}{\sim} N(0, \sigma^2_w)$. Define
  \begin{equation*}
    X_n = \begin{pmatrix}
  Y_n \\
  \phi_2 Y_{n - 1} + \ldots + \phi_r Y_{n - r + 1} + \MApar_{1}w_{n} + \ldots + \MApar_{r - 1}w_{n - r + 2} \\
    \phi_3 Y_{n - 1} + \ldots + \phi_r Y_{n - r + 2} + \MApar_{2}w_{n} + \ldots + \MApar_{r - 1}w_{n - r + 3} \\
    \vdots \\
    \phi_r Y_{n - 1} + \MApar_{r - 1}w_n
  \end{pmatrix} \in \R^r
  \end{equation*}
  \begin{equation*}
  T = \begin{pmatrix}
\phi_1 & 1 & 0 & \ldots & 0 \\
\phi_2 & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & & \ddots & \\
\phi_{r-1} & 0 & \ldots &  & 1 \\
\phi_r & 0 & \ldots & & 0
\end{pmatrix} \in \R^{r \times r}, \quad\quad
Q = \begin{pmatrix}
  1 \\ \MApar_1 \\ \vdots \\ \MApar_{r - 1}
\end{pmatrix} \in \R^r
  \end{equation*}

We can then recover the ARMA model using the following state space formulation:
\begin{align*}
  X_n &= TX_{n - 1} + Qw_n \\
  Y_n &= \begin{pmatrix} 1 & 0 & \ldots & 0\end{pmatrix} X_n
\end{align*}

This results in a linear-Gaussian SSM, and the likelihood function $\mathcal{L}(\paramVec)$ can be evaluated exactly using the Kalman filter \citep{kalman60}.
\begin{itemize}
  \item The likelihood can be maximized by combining this with a numeric optimizer \citep{gardner1980}.
\end{itemize}

This approach has been the standard method for fitting ARIMA models since the early 2000's due to modern computing capabilities \citep{ripley2002}. 

\end{frame}

\begin{frame}{Optimization Shortcomings}
\setbeamercovered{transparent}
This existing approach frequently results in sub-optimal parameter estimates. Simple example in \code{R}: 
\begin{itemize}
  \item Generate data from an $\mathrm{ARMA}(2, 2)$ model.
  \item Fit both $\mathrm{ARMA}(2, 1)$ and $\mathrm{ARMA}(2, 2)$ models to simulated data.
\end{itemize}
The $\mathrm{ARMA}(2, 1)$ is formally a special case of an $\mathrm{ARMA}(2, 2)$ model, with $\MApar_2 = 0$. \pause
Simulation details: 
\begin{itemize}
  \item $(\phi_1, \phi_2, \MApar_1, \MApar_2) = (0.2, -0.1, 0.4, 0.2)$.
  \item $w_n \overset{\text{iid}}{\sim} N(0, 1)$. 
  \item $N = 100$ observations with intercept $\mu = 13$ so that $E[Y_n] \neq 0$.
\end{itemize}



\end{frame}

\begin{frame}{Fitting ARMA models}
\setbeamercovered{transparent}

The \citet{gardner1980} is the standard method for fitting ARMA model parameters. It is implemented in the base \code{stats} package in R, as well as the \code{statsmodels} module in Python.

% Now we fit both both classes of models: $\mathrm{ARMA}(2, 1)$ and $\mathrm{ARMA}(2, 2)$: 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{mod1} \hlkwb{<-} \hldef{stats}\hlopt{::}\hlkwd{arima}\hldef{(y,} \hlkwc{order} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{2}\hldef{,} \hlnum{0}\hldef{,} \hlnum{1}\hldef{))}

\hldef{mod2} \hlkwb{<-} \hldef{stats}\hlopt{::}\hlkwd{arima}\hldef{(y,} \hlkwc{order} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{2}\hldef{,} \hlnum{0}\hldef{,} \hlnum{2}\hldef{))}
\end{alltt}
\end{kframe}
\end{knitrout}

\pause 
The likelihood of \code{mod1} is -141.2, and the likelihood of \code{mod2} is -144.3. The \alert{smaller} model has a log-likelihood that is 3.1 units \alert{higher} than the larger model, which is mathematically impossible under proper optimization (could estimate $\hat{\MApar}_2 = 0$).
\end{frame}

\begin{frame}{Convergence to local optima}
  Why? Likelihood surface is often multimodal, and existing procedures run the risk of converging to a local solution \citep{ripley2002}. Example $\mathrm{MA}(1)$ profiles:


  

\begin{figure}[ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=0.9\maxwidth]{figure/mutliMode-fig-1} 
\end{knitrout}
\caption{\label{fig:multiMode}The profile log-likelihood of data simulated from four distinct MA(1) models. The dashed, red line indicates the true value of $\MApar_1$; the short-dashed blue line is the CSS-initialization. The solid black line correspond to the estimate $\hat{\MApar}_1$ using \code{stats:arima}.}
\end{figure}

\end{frame}

\begin{frame}{Multiple parameter initializations}
\setbeamercovered{transparent}
  In other contexts with multi-model loss functions, the optimization is often repeated using multiple initializations.
  
  However, I have seen \alert{no instances} of this for $\mathrm{ARIMA}$ models. There are a few challenges: 
  \begin{itemize}
    \item Most users don't know about the possibility of converging to local solutions. 
    \item Their are complex constraints on parameter initializations. 
    \begin{itemize}
      \item Constraints are on the roots of polynomials formed by model parameters, not directly on parameters themselves; transformations not readily available.
    \end{itemize}
  \end{itemize}
  \pause
  The roots of the polynomials $\Phi(x) = 1 - \phi_1 x - \phi_2x^2 - \ldots - \phi_px^p$ and $\Psi(x) = 1 + \MApar_1 x + \MApar_2x^2 + \ldots + \MApar_qx^q$ must lie outside the complex unit circle.
\end{frame}

\begin{frame}
  For parameters to be real, the roots need to be sampled as real or conjugate pairs. 
  
  We cannot sample all roots as conjugate pairs (or real), as this would result in specific parameters being all one sign. 
  
  Our approach for each root is the following:
  \begin{itemize}
    \item Sample inverted-root magnitudes uniformly $U(\gamma, 1-\gamma)$.
    \item With probability $p = \sqrt{1/2}$, sample inverted-root pairs as real.
    \begin{itemize}
      \item If real, assign the same sign with probability $p$.
      \item If complex, sample angle from $U(0, \pi)$, and use to assign conjugate pairs of inverted-roots.
    \end{itemize}
    \item With roots sampled, calculate corresponding coefficients and perform optimization routine.
    \item Repeat until convergence.
  \end{itemize}
\end{frame}

\begin{frame}{Revisiting toy example}
\setbeamercovered{transparent}

Algorithm allows for minimal changes to user interface.
Using the same data, models fit using the \code{arima2} package:



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{mod1v2} \hlkwb{<-} \hldef{arima2}\hlopt{::}\hlkwd{arima}\hldef{(y,} \hlkwc{order} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{2}\hldef{,} \hlnum{0}\hldef{,} \hlnum{1}\hldef{))}

\hldef{mod2v2} \hlkwb{<-} \hldef{arima2}\hlopt{::}\hlkwd{arima}\hldef{(y,} \hlkwc{order} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{2}\hldef{,} \hlnum{0}\hldef{,} \hlnum{2}\hldef{))}
\end{alltt}
\end{kframe}
\end{knitrout}

With this new algorithm and software, the likelihood of \code{mod1v2} is -141.173, and the likelihood of \code{mod2v2} is -141.172.

\pause
The likelihood of the smaller model was unchanged, but the larger model had an increase in log-likelihood of 3.1. The likelihoods of the nested models are now \alert{consistent}.
\end{frame}

\begin{frame}{Summary}
\setbeamercovered{transparent}
$\mathrm{ARMA}$ models are not necessarily state-of-the art. Why should we care?
  \begin{itemize}
    \item $\mathrm{ARMA}$ models are among the most frequently used approaches in all of statistics, so even small improvements are worth the effort.\pause
    \item Software that claims to maximize model likelihoods fails to do so in a large number of cases ($>20\%$).\pause
    \item $\mathrm{ARMA}$ models are often used in conjunction with linear regression. Likelihood ratio tests are common for testing the inclusion / significance of regression parameters.\pause
    \begin{itemize}
      \item Typical improvements in log-likelihood in the range $(0.22, 1.46)$. This shortcoming in one or both model is large enough to change the outcome of these tests.\pause
    \end{itemize}
    \item Even if outcomes are unchanged, confidence that software / algorithms will reliably do what you expect is important. 
  \end{itemize}
\end{frame}

\end{document}
%EoF
