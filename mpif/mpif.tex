\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usetheme{gotham}

	\usepackage{standalone}
	\usepackage{tikz}
	\usepackage{pgfplots}
	\usepackage{tabularray} % Typeset tabulars and arrays (contains equivalent of longtable, booktabs and dcolumn at least)
		\UseTblrLibrary{booktabs} % to load extra commands from booktabs
	\usepackage{changepage}
	\usepackage{minted}
	\usepackage[ruled,noline,linesnumbered]{algorithm2e}
		\definecolor{codeback}{rgb}{0.90,0.91,0.92}
		\definecolor{codebackdark}{rgb}{0.10,0.11,0.12}

	\newcommand{\famName}[1]{\textsc{#1}}
	\newcommand{\themename}{\textbf{\textsc{Gotham}}}



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\section{The Marginalized Panel Iterated Filter (MPIF)}

\begin{frame}{Motivation}
Often we have a collection of related time series called, called \emph{panel data}.
We want to make inference using the entire collection, not just on each time series.

Examples:
  \begin{itemize}
    \item Model for disease outbreaks of the same disease, different locations (hospitals / cities) \citep{lee20}.
    \item Experiments / observational studies on ecological populations \citep{searle16}.
    \item Longitudinal studies using within-host dynamic models \citep{ranjeva17}.
  \end{itemize}
  
  Mechanistic models seldom used for panel data, despite widespread availability.
  
  % This suggests that the practical difficulties for existing procedures. 
\end{frame}

\begin{frame}{Panel models}
  Measurements for unit $u$ taken at times $t_{u, 1:N_u}$. Observed and latent process at these times denoted $Y_{u, n}$ and $X_{u, n}$, respectively.
  
  Each unit $u$ defines an independent POMP model, the entire collection of models is a PanelPOMP.
  
  $$
  \mathcal{L}(\paramVec; \bm{y}^*) = \int \prod_{u = 1}^U f_{X_{u, 0}}\big(x_{u, 0};\, \paramVec\big)\prod_{n = 1}^{N_u} f_{X_{u, n}|X_{u, n-1}}\big(x_{u, n}|x_{u, n-1}; \, \paramVec\big)f_{Y_{u, n}|X_{u, n}}\big(y_{u, n}|x_{u, n}; \, \paramVec\big) dx_{1:U,0:N_u}.
  $$
  
  The parameter vector $\theta$ has shared components $\phi$, and unit specific components $\psi_{1:U}$.
  $$\theta = (\phi, \psi_{1:U})$$
  
\end{frame}

\begin{frame}{The problem}

  Independent models, why not do inference independently? 

  \begin{itemize}
    \item Each model may share features (or parameters), and we want to estimate using all of the data. 
  \end{itemize}
  Common inference procedures in low dimensions rely on particle filters \citep{arulampalam02}.
  \begin{itemize}
    \item[{\color{green} $\checkmark$}] Particle filters work in low-dimensions, can be applied independently to units.
    \item[{\color{red} $X$}] Iterated filtering (IF) is an extension used to perform maximum likelihood estimation \citep{ionides15}.
    \begin{itemize}
      \item IF introduces dependence because of shared $\theta$, making it a high-dimensional problem.
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Background: Data cloning and iterated filtering}
  
  IF is a special type of Data cloning \citep{lele07}. 
  
  Denote $\pi_i(\theta)$ as the posterior distribution of the parameter vector $\theta$ after the $i$th Bayesian update, and $\mathcal{L}(\paramVec; \bm{y}^*)$ as the likelihood
\begin{align*}
\pi_1(\theta) &\propto \mathcal{L}(\paramVec; \bm{y}^*)\pi_0(\theta), \\
\pi_2(\theta) &\propto \mathcal{L}(\paramVec; \bm{y}^*)\pi_1(\theta) \propto \mathcal{L}(\paramVec; \bm{y}^*)^2\pi_0(\theta),\\
&\vdots\\
\pi_m(\theta) &\propto \mathcal{L}(\paramVec; \bm{y}^*)^m\pi_0(\theta).
\end{align*}
If we let $m\rightarrow \infty$, the effect of the initial prior distribution diminishes, and the $m$th posterior has all of its mass centered at the MLE.

\end{frame}

\begin{frame}{Iterated filtering}
\setbeamercovered{transparent}
  Loosely speaking, iterated filtering is just data cloning with the additional pieces: 
  \begin{enumerate}
    \item Likelihood is approximated using sequential Monte Carlo. 
    \item Parameter particles are perturbed over time, treated like states.
    % \item Parameter particles reweighted using conditional log-likelihoods.
  \end{enumerate}\pause
  {\color{green} $\checkmark$} The perturbation of parameters is necessary to avoid particle depletion, a known problem with particle filters + Bayesian inference \citep{chen24}.
  
  {\color{red} $X$} The perturbations introduce a loss of information \citep{liu01}, so are decreased over the cloning iteration.
\end{frame}

\begin{frame}[plain, t]{Iterated Filtering for Panel Models}
\begin{columns}[t]
\begin{column}{0.69\textwidth}
  \begin{algorithm}[H]
  \notsotiny
\For{$m \in 1:M$}{
  Set $\Theta_{0, j}^{F, m} = \Theta_{j}^{m-1} = (\Phi_j^{m-1}, \Psi_{1:U, j}^{m-1})$ for $j \in \seq{1}{J}$\;
    \For{$u \in \seq{1}{U}$}{
        Set $\Theta_{u, 0, j}^{F, m} = (\Phi_{u, 0, j}^{F, m}, \Psi_{1:U, 0, j}^{F, m}) \sim h_{u, 0}\big(\cdot | \Theta^{F, m}_{u-1, j}; \sigma_{u, m}\big)$ \label{line:startu}\; 
      Initialize $X_{u, 0, j}^{F, m} \sim f_{X_{u, 0}}(x_{u, 0}; \Phi_{u, 0, j}^{F, m}, \Psi_{u, 0, j}^{F, m})$ for $j \in \seq{1}{J}$\;
        \For{$n \in \seq{1}{N_u}$} {
          Set $\Theta_{u, n, j}^{P, m} = (\Phi_{u, n, j}^{P, m}, \Psi^{P, m}_{1:U, n, j}) \sim h_{u, n}\big(\cdot |\Theta_{u, n-1, j}^{F, m}; \sigma_{u, m}\big)$ for $j \in \seq{1}{J}$ \label{line:perturbations}\;
          $X_{u, n, j}^{P, m} \sim f_{X_{u, n}|X_{u, n-1}}\big(x_{u, n}|X_{u, n-1, j}^{F, m}; \Phi_{u, n, j}^{P, m}, \Psi_{u, n, j}^{P, m}\big)$ for $j \in \seq{1}{J}$ \label{line:Xpred}\;
          $w_{u, n, j}^m = f_{Y_{u, n}|X_{u, n}}\big(y_{u, n}^*|X_{u, n, j}^{P, m};\Phi_{u, n, j}^{P, m}, \Psi_{u, n, j}^{P, m}\big)$ for $j \in \seq{1}{J}$ \label{line:weights}\;
          Draw $k_{\seq{1}{j}}$ with $P(k_j = i) = w_{u, n, i}^m / \sum_{v = 1}^J w_{u, n, v}^m$ for $i, j \in \seq{1}{J}$\;
          Set $X_{u, n, j}^{F, m} = X_{u, n, k_j}^{P, m}$, and $\big(\Phi_{u, n, j}^{F, m}, \Psi_{u, n, j}^{F, m}\big) = \big(\Phi_{u, n, k_j}^{P, m}, \Psi_{u, n, k_j}^{P, m}\big)$ for $j \in \seq{1}{J}$\;
            \uIf{$\mathrm{MARGINALIZE}$}{$\Psi^{F,m}_{\tilde u,n,j} = \Psi^{P,m}_{\tilde u,n,j}$ for all $\tilde u \neq u$, $j=\seq{1}{J}$\label{mpif:update}} 
       \Else{$\Psi^{F,m}_{\tilde u,n,j} = \Psi^{P,m}_{\tilde u,n,k_j}$ for all $\tilde u \neq u$, $j=\seq{1}{J}$}
        }
      Set $\Theta_{u, j}^{F, m} = \big(\Phi_{u, N_u, j}^{F, m}, \Psi_{u, N_{1:U}, j}^{F, m}\big)$ for $j \in \seq{1}{J}$ \label{line:endu}\;
    }
  Set $\Theta_j^{(m)} = \Theta_{U, j}^{F, m}$ for $j \in \seq{1}{J}$\;
}
\end{algorithm}
\end{column}

\begin{column}{0.35\textwidth}
\vspace{-4cm}

\textbf{\underline{Function Inputs:}}

\vspace{2mm}

  \begin{itemize}
    \footnotesize
    \item Initializer: $f_{X_{u, 0}}(x_{u, 0}; \, \theta)$.
    \item Process Simulator: $f_{X_{u, n}|X_{u, n-1}}(x_{u, n}|x_{u, n-1}; \, \theta)$.
    \item Measurement Model: $f_{Y_{u, n}|X_{u, n}}(y_{u, n}|x_{u, n} ; \, \theta)$. 
    \item Data $y_{u, n}^*$. 
    \item Iterations, Particles: $(M, J)$.
    \item Starting parameter swarm, $\Theta_j^0 = \big(\Phi_j^0, \Psi_{1:U, j}^0\big)$.
    \item Perturbation simulator: $h_{u, n}(\cdot | \varphi; \sigma)$.
    \item Variance sequence: $\sigma_{1:U, 1:M}$.
  \end{itemize}
  
\end{column}

\end{columns}
\end{frame}

\begin{frame}{PIF Theory}
  The panel iterated filter (PIF) is a type of IF, mitigating information loss \citep{breto20}. 
  It has been successfully used to estimate the MLE previously \citep[e.g.,][]{domeyer22}.
  
  \begin{theorem}[Chapter 4, Theorem 1]
    Extends theory of \citet{chen24} to panel models. Denote the output of the PIF algorithm as $\Theta_{1:J}^{(M)}$.
  Then there exists some positive sequences $\{C_M\}_{M \geq 1}$ and $\{\epsilon_M\}_{M \geq 1}$ where $\lim_{M \rightarrow \infty}\epsilon_M = 0$ such that for all $(J, M) \in \mathbb{N}^2$,
  $$
  E\bigg[\Big|\frac{1}{J}\sum_{i=1}^J \Theta_j^{(M)} - \hat{\theta}\Big|_2\bigg] \leq \frac{C_M}{\sqrt{J}} + \epsilon_M
  $$
  \end{theorem}
  
\end{frame}

\begin{frame}{Proof sketch and discussion}
  Conditions:
  \begin{itemize}
    \item On parameter space $\Theta$: Compact, corners not too ``sharp" \citep[regular compact set, Def~1 of][]{chen24}.
    \item Regularity conditions on the model (positive and finite likelihood, densities are smooth functions of $\theta$). 
    \item Conditions on the parameter perturbations (type of perturbations, and cooling schedule). 
  \end{itemize}
  Proof Sketch: 
  \begin{itemize}
    \item \citet{chen24} provides theory for convergence of IF of POMP models; we write a general PanelPOMP as a POMP model, and PIF is a special case of IF for these models. 
  \end{itemize}
\end{frame}

\begin{frame}{Iterated Filtering for panel models: a tradeoff}
  Iterated filtering is done one observation at a time. 
  In the panel setting, this means we also process one unit $u$ at a time.
  
  Ignoring perturbations, we have do \emph{unit} data cloning, iterating over $(m, u)$: 
  \begin{align}
    \pi_{m, u}(\theta) &\propto \mathcal{L}(\paramVec; y_u^*)\, \pi_{m, u-1}(\theta) = \mathcal{L}_u(\phi, \psi_u; y_u^*)\, \pi_{m, u-1}(\theta), \label{eq:PIFupdate}
  \end{align}
    Using $\pi_{0, 0}(\theta)$ as the initial prior distribution. Parameter dependence in posteriors introduced by iterating Eq.~\ref{eq:PIFupdate} over $u$.
    
    Two options of iterated filtering: 
    \begin{itemize}
      \item Perturb all parameters at each time step (IF2, high loss of information). 
      \item When using data from unit $u$, only perturb $\phi$ and $\psi_u$ (PIF, high particle depletion). 
    \end{itemize}
\end{frame}

\begin{frame}{MPIF motivation}
\setbeamercovered{transparent}
  If the previous prior $\pi_{m, u-1}(\theta)$ has parameter independence: $\pi_{m, u-1}(\theta) = f(\psi_{-u})g(\phi, \psi_{u})$, then there is no need to resample particles $\psi_{-u}$. 
  
  This would avoid the particle depletion \emph{and} loss-of-information, and motivates marginalized data cloning (repeating Eqs.~\ref{eq:margBayes}--\ref{eq:MPIFupdate}).
  
  \begin{align}
\tilde{\pi}_{m, u}(\theta) &\propto \mathcal{L}_{u}(y^*_u;\, \phi, \psi_u)\, \pi_{m, u-1}(\theta) \label{eq:margBayes}\\
\pi_{m, u}(\theta) &\propto \int \! \tilde{\pi}_{m, u}(\theta) \, d\phi \, d\psi_u \, \times \int \! \tilde{\pi}_{m, u}(\theta) \, d\psi_{-u} \label{eq:MPIFupdate}.
\end{align}\pause

Marginalization can happen in various ways. Next slide is a figure with bivariate Gaussian densities, marginalized over all parameters. 

\end{frame}

\begin{frame}{Marginalized Bayes: Guassian figure}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/figDC-1} 

}


\end{knitrout}

\end{frame}

\begin{frame}{MPIF theory}

  Just as IF extends data cloning (perturbations + particle approximation), the MPIF algorithm extends marginalized data cloning. 

  \begin{itemize}
    \item Existing theory for IF algorithms rely heavily on the data cloning principle \cite{ionides15, chen24}.
    \item The non-linearity introduced by the marginalization step invalidates these approaches.
    % \item Existing theory for IF algorithms cannot readily be extended to MPIF, because of the non-linearity introduced by the marginalization step. 
    \item A natural first question is whether or not marginalized data cloning converges.
    \begin{itemize}
      \item Unfortunately, a few toy examples suggests not always (computationally and analytically).
    \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}{Marginalized data cloning: Gaussian likelihoods}
  Convergence is explored via Gaussian likelihoods.
  
  The properties of this special case is relevant to the broader class of models that is well approximated by Gaussian models, (e.g., local asymptotic normality \citep{lecam00}).
  
  \begin{theorem}[Chapter 4, Theorem 2]
    Let $\mathcal{L}_u(y_u^*; \, \phi, \psi_u)$ be the likelihood that corresponds to a Gaussian distribution with mean $(\phi^*, \psi_u^*)$ and precision $\Lambda_u^* \in \R^{2\times2}$. Under suitable conditions on the matrices $\Lambda^*_u$, then if the initial prior density is Gaussian, then the density of the $m$th iteration of Eq.~\ref{eq:MPIFupdate} converges to a point mass at the MLE $(\phi^*, \psi_1^*, \ldots, \psi^*_U)$ as $m\rightarrow \infty$.
  \end{theorem}
\end{frame}

\begin{frame}{Gaussian Theory: Proof sketch}

\begin{itemize}
  \item Gaussian priors + Gaussian likelihoods imply Gaussian posteriors.
  \item Transform data so likelihood is centered at zero.
  \item The marginalization step only modifies the covariance, setting off-diagonal terms to zero. Conditions ensure this loss of information is not too large.
  \item Diagonals of covariance shrink to zero asymptotically at rate $1/m$.
  \item Each unit-iteration updates the $\phi$ and $\psi_u$ components of $\mu_m$.
  \item $\mu_m = \big(\prod_{i = 1}^m\prod_{u = 1}^U A_{u, i}\big) \mu_0 = \big(\prod_{i = 1}^m P_m\big) \mu_0$, with $\|P_m\|_2 = 1 - \epsilon_m/m + o(1/m)$, with $\epsilon_m$ positive, bounded.
\end{itemize}

\end{frame}

\begin{frame}{Convergence with perturbations}

    \begin{theorem}[Chapter 4, Corollary 1]
    Consider the same setup as Theorem~2 (Chapter~4). If parameters are perturbed prior performing the Bayes update at each step using Gaussian additive noise with mean $0$ and covariance $\sigma^2_m\Sigma_0$, then if $\sigma_m^2 = o(1/m)$, the algorithm still converges to the MLE as $m\rightarrow \infty$. 
  \end{theorem}
  
  \begin{itemize}
    \item Useful heuristic: a more dispersed prior typically results in a posterior distribution closer to the likelihood function.
    \item Adding Gaussian noise at each step results in larger updates towards to MLE at each step.
    \item Heuristically, convergence of unperturbed case implies convergence of perturbed case, if perturbation variance decreases to zero.
  \end{itemize}

\end{frame}

\begin{frame}{The MPIF advantage: improved particle representation}

  One perspective is that the marginalization adds a small amount of bias in each intermediate posterior distribution.
  
  \begin{itemize}
    \item Theorem~2 (Chapter 4) gives formal conditions where the bias at each step is small, and algorithm converges to MLE (no bias).
    \item The advantage is improved particle representations. In this case, MPIF may still be preferable even if there is small bias.
  \end{itemize}
  
  Thus, we can think of MPIF as improving a bias-variance tradeoff.
  
  \begin{itemize}
    \item To demonstrate this, perform a single unit-iteration of PIF and MPIF on Gaussian data and model.
  \end{itemize}
  
\end{frame}

\begin{frame}{Bias-Variance tradeoff}
  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.65\maxwidth]{figure/unnamed-chunk-1-1} 

}


\end{knitrout}
  
\end{frame}

\begin{frame}{Gompertz population model: high-dimensional space}

The MPIF algorithm is demonstrated on data simulated from a stochastic population model. We have $U$ ranging from $5--2500$, and $N_u = N \in \{20, 50, 100\}$.
\begin{itemize}
  \item $X_{u, n+1} = K^{1- \exp{r_u}}_u \epsilon_{u, n}$, $\epsilon_{u, n} \sim N(0, \sigma^2_u)$. 
  \item $Y_{u, n} | X_{u, n} \sim N\big(\log X_{u, n}, \tau^2_u\big)$
  \item Fix $K_u = 1$, $X_{u, 0} = 1$. Estimate $\sigma_u^2 = \sigma^2 = 0.01$, $r_u = r = 0.1$, and $\tau_u^2 = 0.01$ for all $u$.  
\end{itemize}

This is log-linear Gaussian, so exact likelihood can be computed \citep{kalman60}. Parameter choices match \citet{breto20}. 

\end{frame}

\begin{frame}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/gompFig1-1} 

}


\end{knitrout}
\end{frame}

\begin{frame}{Discussion}
  \begin{itemize}
    \item In all tested models, the MPIF algorithm outperforms PIF (especially as $U$ and $d_\psi$ grow).
    \item Even poor performing replicates of MPIF often outperform PIF, despite weaker theoretical guarantees.
    \item Improved performance is a result of reduced particle depletion.
    \item Stronger theory for MPIF is available in some special cases, each covered by Theorem~1 of Chapter~4:
    \begin{itemize}
      \item No shared parameters: equivalent to performing IF2 independently to each model. 
      \item No unit-specific parameters: MPIF is the same as PIF.
    \end{itemize}
  \end{itemize}
\end{frame}

\end{document}
%EoF
