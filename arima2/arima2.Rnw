\documentclass[aspectratio=169]{beamer}
\usetheme{gotham}

	\usepackage{standalone}
	\usepackage{tikz}
	\usepackage{pgfplots}
	\usepackage{tabularray} % Typeset tabulars and arrays (contains equivalent of longtable, booktabs and dcolumn at least)
		\UseTblrLibrary{booktabs} % to load extra commands from booktabs
	\usepackage{changepage}
	\usepackage{minted}
		\definecolor{codeback}{rgb}{0.90,0.91,0.92}
		\definecolor{codebackdark}{rgb}{0.10,0.11,0.12}

	\newcommand{\famName}[1]{\textsc{#1}}
	\newcommand{\themename}{\textbf{\textsc{Gotham}}}

<<arima2Setup>>=
library(arima2)
library(tidyverse)
library(latex2exp)
root <- ""
@


\begin{document}

\section{Likelihood Maximization for ARMA models}

\begin{frame}{Auto-regressive moving average ($\mathrm{ARMA}$) models}
  $\mathrm{ARMA}$ models are the most frequently used approach to modeling time series data.
  % ARMA models are as foundational to time series analysis as linear models are to regression analysis, and they are often used in conjunction for regression with ARMA errors.
  
  \begin{block}{ARMA model definition}
    A time series $Y_{1:N}$ is called $\mathrm{ARMA}(p, q)$ if it is (weakly) stationary and
    \begin{equation}
    Y_n = \phi_1Y_{n - 1} + \cdots + \phi_pY_{n - p} + w_n + \MApar_1w_{n-1} + \ldots + \MApar_qw_{n - q},\label{eq:arma}
    \end{equation}
    with $\{w_n; n = 0, \pm1, \pm2, \ldots\}$ denoting a mean zero white noise (WN) processes with variance $\sigma_w^2 > 0$, and $\phi_p \neq 0$, $\MApar_q \neq 0$.
  \end{block}
  
  $p$ and $q$ of Eq.~\myeqref{eq:arma} as the autoregressive (AR) and moving average (MA) orders, respectively.
\end{frame}

\begin{frame}[allowframebreaks]{State space formulation}
  How are they relevant? Inference methodology treats $\mathrm{ARMA}$ models as \emph{non-mechanistic} SMMs. Let $r = \max(p, q+1)$, and assume $w_n \overset{\mathrm{iid}}{\sim} N(0, \sigma^2_w)$. Define
  \begin{equation*}
    X_n = \begin{pmatrix}
  Y_n \\
  \phi_2 Y_{n - 1} + \ldots + \phi_r Y_{n - r + 1} + \MApar_{1}w_{n} + \ldots + \MApar_{r - 1}w_{n - r + 2} \\
    \phi_3 Y_{n - 1} + \ldots + \phi_r Y_{n - r + 2} + \MApar_{2}w_{n} + \ldots + \MApar_{r - 1}w_{n - r + 3} \\
    \vdots \\
    \phi_r Y_{n - 1} + \MApar_{r - 1}w_n
  \end{pmatrix} \in \R^r
  \end{equation*}
  \begin{equation*}
  T = \begin{pmatrix}
\phi_1 & 1 & 0 & \ldots & 0 \\
\phi_2 & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & & \ddots & \\
\phi_{r-1} & 0 & \ldots &  & 1 \\
\phi_r & 0 & \ldots & & 0
\end{pmatrix} \in \R^{r \times r}, \quad\quad
Q = \begin{pmatrix}
  1 \\ \MApar_1 \\ \vdots \\ \MApar_{r - 1}
\end{pmatrix} \in \R^r
  \end{equation*}

We can then recover the $\mathrm{ARMA}$ model using the following state space formulation:
\begin{align*}
  X_n &= TX_{n - 1} + Qw_n \\
  Y_n &= \begin{pmatrix} 1 & 0 & \ldots & 0\end{pmatrix} X_n
\end{align*}

This results in a linear-Gaussian SSM, and the likelihood function $\mathcal{L}(\paramVec)$ can be evaluated using the Kalman filter \citep{kalman60}.
\begin{itemize}
  \item The likelihood can be maximized by combining this with a numeric optimizer \citep{gardner1980}.
\end{itemize}

This approach has been the standard method for fitting ARIMA models since the early 2000's \citep{ripley2002}. 

\end{frame}

\begin{frame}{Optimization Shortcomings}
\setbeamercovered{transparent}
This existing approach frequently results in sub-optimal parameter estimates. Simple example in \code{R}: 
\begin{itemize}
  \item Generate data from an $\mathrm{ARMA}(2, 2)$ model.
  \item Fit both $\mathrm{ARMA}(2, 1)$ and $\mathrm{ARMA}(2, 2)$ models to simulated data.
\end{itemize}
The $\mathrm{ARMA}(2, 1)$ is a special case of an $\mathrm{ARMA}(2, 2)$ model, with $\MApar_2 = 0$. \pause
Simulation details: 
\begin{itemize}
  \item $(\phi_1, \phi_2, \MApar_1, \MApar_2) = (0.2, -0.1, 0.4, 0.2)$.
  \item $w_n \overset{\text{iid}}{\sim} N(0, 1)$. 
  \item $N = 100$ observations with intercept $\mu = 13$ so that $E[Y_n] \neq 0$.
\end{itemize}

<<armaSeed, echo=FALSE,include=FALSE,comment=FALSE>>=
set.seed(839076)
y <- 13 + arima.sim(
  n = 100, model = list(ar = c(0.2, -0.1), ma = c(0.4, 0.2))
)
@

\end{frame}

\begin{frame}{Fitting ARMA models}
\setbeamercovered{transparent}

The \citet{gardner1980} is the standard method for fitting $\mathrm{ARMA}$ parameters. 
It is implemented in the base statistics libraries in \code{R} and \code{Python}.

% Now we fit both both classes of models: $\mathrm{ARMA}(2, 1)$ and $\mathrm{ARMA}(2, 2)$: 
<<fitARMA, echo=TRUE>>=
mod1 <- stats::arima(y, order = c(2, 0, 1))

mod2 <- stats::arima(y, order = c(2, 0, 2))
@

\pause 
The likelihood of \code{mod1}, \code{mod2} is \Sexpr{round(mod1$loglik, 1)} and \Sexpr{round(mod2$loglik, 1)}, respectively. The \alert{smaller} model has log-likelihood \Sexpr{round(mod1$loglik - mod2$loglik, 1)} units \alert{higher} than the larger model, which is impossible under proper optimization (could estimate $\hat{\MApar}_2 = 0$).
\end{frame}

\begin{frame}{Convergence to local optima}
  Why? Likelihood surface is often multimodal, and existing procedures run the risk of converging to a local solution \citep{ripley2002}. Example $\mathrm{MA}(1)$ profiles:

<<MultiModeSetup, echo=FALSE>>=
sample_data <- function(data, job, n, P, Q, ...) {

  res <- list()

  coefs <- sample_ARMA_coef(
    order = c(P, Q), Mod_bounds = c(0.1, 0.9), min_inv_root_dist = 0.1
  ) |> round(digits = 2)

  res$ars <- coefs[grepl("^ar[[:digit:]]+", names(coefs))]
  res$mas <- coefs[grepl("^ma[[:digit:]]+", names(coefs))]

  res$x <- arima.sim(
    n = n,
    model = list(ma = res$mas)
  )

  res
}

checkImprove <- function(data, job, instance, ...) {

  arma2_fit <- tryCatch(
    arima2::arima(
      instance$x, order = c(0, 0, 1), SSinit = 'Rossignol2011', max_inv_root = 0.99,
      max_repeats = 10, include.mean = FALSE
    ),
    error = function(e) list(num_starts = 0, coef = c('ma1' = 0))
  )

  c('num_starts' = arma2_fit$num_starts, 'll_improve' = ifelse(arma2_fit$num_starts > 10, arma2_fit$all_values[arma2_fit$num_starts] - arma2_fit$all_values[1], NA_real_), "truth" = instance$mas, "est" = arma2_fit$coef)
}

get_profile <- function(model, npts) {
  results <- matrix(ncol = 2, nrow = npts)

  fit_arma <- function(fixed) {

    P <- model$arma[1]
    Q <- model$arma[2]

    arima2::arima(
      model$x, order = c(0, 0, 1), fixed = c(fixed), SSinit = 'Rossignol2011',
      max_iters = 1, include.mean = FALSE
    )$loglik
  }

  fixed_vals <- seq(-1+1e-8, 1-1e-8, length.out = npts)
  lls <- purrr::map_dbl(fixed_vals, fit_arma)
  results[, 1] <- fixed_vals
  # results[, 2] <- model$coef[2]
  results[, 2] <- lls
  colnames(results) <- c(names(model$coef), 'loglik')
  as.data.frame(results)
}

ma1_res <- readRDS(paste0(root, "data/sim_MA1_results.rds"))

ma1_improve <- ma1_res %>%
  filter(num_starts > 10) %>%
  filter(abs(est.ma1) < 0.95)

exp_seed <- 100000
prob_seed <- 50000

# Loop --------------------------------------------------------------------

job_ids <- c(10463, 16385, 22383, 41307)
ggs <- list()

for (j in 1:length(job_ids)) {
  i <- job_ids[j]
  # i <- test_mods[j]
  set.seed(prob_seed + (i %% 25000) - 1)

  instance <- sample_data(
    NULL,
    i,
    pull(ma1_res, n)[i],
    P = 0, Q = 1
  )

  set.seed(exp_seed + i)

  arma2_fit <- tryCatch(
    arima2::arima(
      instance$x, order = c(0, 0, 1), SSinit = 'Rossignol2011', max_inv_root = 0.99,
      max_repeats = 10, include.mean = FALSE
    ),
    error = function(e) list(num_starts = 0)
  )

  stats1 <- stats::arima(
    instance$x,
    order = c(0, 0, 1),
    SSinit = 'Rossignol2011',
    include.mean = FALSE
  )

  stats2 <- stats::arima(
    instance$x,
    order = c(0, 0, 1),
    SSinit = 'Rossignol2011',
    include.mean = FALSE,
    method = 'CSS'
  )

  mod_prof <- get_profile(arma2_fit, 1000)
  # mod_prof <- profile(arma2_fit, which = 1, lower = -1 + 1e-8, upper = 1 - 1e-8, npts = 1000)

  diffs <- case_when(
    j == 1 ~ 0.9,
    j == 2 ~ 3,
    j == 3 ~ 0.5,
    j == 4 ~ 0.5,
    TRUE ~ 2
  )

  df_lines <- data.frame(
    'value' = c(
      instance$mas,
      arma2_fit$coef[1],
      stats1$coef[1],
      stats2$coef[1]
    ),
    'type1' = c('truth', 'est', 'est', 'init'),
    'type2' = c('truth', 'est1', 'est2', 'truth')
  )
  
  df_lines <- df_lines %>% 
    filter(type2 != 'est1')

  gg_tmp <- mod_prof %>%
  filter(loglik > max(loglik) - diffs) %>%
  ggplot(aes(x = ma1, y = loglik)) +
  geom_line() +
  geom_vline(data = df_lines, aes(xintercept = value, col = type1, linetype = type1)) +
  # geom_vline(xintercept = instance$mas, col = 'black') +
  # geom_vline(xintercept = stats2$coef, col = 'black', linetype = 'dotted') +
  theme_bw() +
  labs(
    x = TeX('$\\varphi_1$'),
    y = "Loglikelihood"
  ) +
  theme(plot.title = element_text(hjust = 0.5)) +
  # Add manual legend using dummy data
  scale_linetype_manual(
    name = NULL,
    values = c(
      "truth" = "longdash",
      "est" = "solid",
      "init" = "dashed"
    ), labels = c("truth" = 'Truth', "est" = "Estimate", "init" = TeX('$\\varphi_1$ Initialization'))
  ) +
  scale_color_manual(
    name = NULL,
    values = setNames(c("red", "black", "blue"), c("truth", "est", 'init')),
    labels = c("truth" = 'Truth', "est" = "Estimate", "init" = TeX('$\\varphi_1$ Initialization'))
    # labels = setNames(c("arima2::arima", "stats::arima"), c("est1", "est2"))
  ) +
  # guides(
  #   linetype = guide_legend(order = 1),
  #   color = guide_legend(order = 2)
  # ) +
  theme(legend.title = element_blank(), legend.margin = margin(c(-2,0,0,0)),
        legend.key.spacing.y = unit(10, "pt"), legend.spacing.y = unit(35, 'pt'))

  if (j == 2 | j == 4) {
    gg_tmp <- gg_tmp + theme(axis.title.y = element_blank())
  }

  if (j == 1 | j == 2) {
    gg_tmp <- gg_tmp + theme(axis.title.x = element_blank())
  }

  ggs[[j]] <- gg_tmp
}
@
  

\begin{figure}[ht]
<<mutliMode-fig, fig.height=2.8, fig.width=6, echo=FALSE, out.width='0.9\\maxwidth'>>=
ggpubr::ggarrange(
  plotlist = ggs,
  legend = 'right',
  common.legend = TRUE
)
@
\caption{\label{fig:multiMode}The profile log-likelihood of data simulated from four distinct MA(1) models. The dashed, red line indicates the true value of $\MApar_1$; the short-dashed blue line is the CSS-initialization. The solid black line correspond to the estimate $\hat{\MApar}_1$ using \code{stats:arima}.}
\end{figure}

\end{frame}

\begin{frame}{Multiple parameter initializations}
\setbeamercovered{transparent}
  In other contexts with multi-model loss functions, the optimization is often repeated using multiple initializations.
  
  However, I have seen \alert{no instances} of this for $\mathrm{ARIMA}$ models. There are a few challenges: 
  \begin{itemize}
    \item Most users don't know about the possibility. 
    \item Their are complex constraints on parameter initializations. 
    \begin{itemize}
      \item Constraints are on the roots of polynomials formed by parameters, not parameters themselves; transformations not readily available.
    \end{itemize}
  \end{itemize}
  \pause
  The roots of the polynomials $\Phi(x) = 1 - \phi_1 x - \phi_2x^2 - \ldots - \phi_px^p$ and $\Psi(x) = 1 + \MApar_1 x + \MApar_2x^2 + \ldots + \MApar_qx^q$ must lie outside the complex unit circle.
\end{frame}

\begin{frame}
  For parameters to be real, the roots need to be real or conjugate pairs. 
  
  We cannot sample all roots as conjugate pairs (or real), as all parameter initializations would have same sign.
  
  Our approach for each root:
  \begin{itemize}
    \item Sample inverted-root magnitudes uniformly $U(\gamma, 1-\gamma)$.
    \item With probability $p = \sqrt{1/2}$, sample inverted-root pairs as real.
    \begin{itemize}
      \item If real, assign the same sign with probability $p$.
      \item If complex, sample angle from $U(0, \pi)$, and use to assign conjugate pairs of inverted-roots.
    \end{itemize}
    \item With roots sampled, calculate corresponding coefficients and perform optimization routine.
    \item Repeat until convergence.
  \end{itemize}
\end{frame}

\begin{frame}{Revisiting toy example}
\setbeamercovered{transparent}

Algorithm allows for minimal changes to user interface.
Using the same data, models fit using the \code{arima2} package:

<<ARMAsetSeed2, echo=FALSE>>=
set.seed(746168)
@

<<fitARMA2, echo=TRUE, >>=
mod1v2 <- arima2::arima(y, order = c(2, 0, 1))

mod2v2 <- arima2::arima(y, order = c(2, 0, 2))
@

With this new algorithm and software, the likelihood of \code{mod1v2} is \Sexpr{round(mod1v2$loglik, 3)}, and the likelihood of \code{mod2v2} is \Sexpr{round(mod2v2$loglik, 3)}.

\pause
The likelihood of the smaller model was unchanged, but the larger model had an increase in log-likelihood of \Sexpr{round(mod2v2$loglik - mod2$loglik, 1)}. The likelihoods of the nested models are now \alert{consistent}.
\end{frame}

\begin{frame}{Summary}
\setbeamercovered{transparent}
$\mathrm{ARMA}$ models are not necessarily state-of-the art. Why should we care?
  \begin{itemize}
    \item $\mathrm{ARMA}$ models are among the most frequently used approaches in all of statistics, so even small improvements are worth the effort.\pause
    \item Software that claims to maximize model likelihoods fails to do so in a large number of cases ($>20\%$).\pause
    \item $\mathrm{ARMA}$ models are often used in conjunction with linear regression. Likelihood ratio tests are common for testing the inclusion / significance of regression parameters.\pause
    \begin{itemize}
      \item Typical improvements in log-likelihood in the range $(0.22, 1.46)$. This shortcoming in one or both model is large enough to change the outcome of these tests.\pause
    \end{itemize}
    \item Even if outcomes are unchanged, confidence that software / algorithms will reliably do what you expect is important. 
  \end{itemize}
\end{frame}

\end{document}
%EoF
